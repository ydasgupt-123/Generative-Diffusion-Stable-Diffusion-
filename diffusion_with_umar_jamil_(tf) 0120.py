# -*- coding: utf-8 -*-
"""Diffusion with Umar Jamil (TF).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17Ax3CSVBki0MYhHe37TMl7ik5XAURIk8
"""

pip install tensorflow keras pillow transformers tqdm huggingface_hub

pip install keras_cv

pip install torch

## Creating the Back-bone for the attention mechanism using the Self Attention and Cross Attention
import tensorflow as tf
import keras
from tensorflow.keras.layers import Input, Dense, Concatenate, Flatten
from tensorflow.keras.models import Model, Sequential


class SelfAttention(tf.keras.layers.Layer):
  def __init__(self, n_heads: int, d_embed: int, in_proj_bias = True, out_proj_bias = True):
    super().__init__()
    # Corrected Dense layer initialization: units should be 3 * d_embed
    self.in_proj_bias = tf.keras.layers.Dense(3 * d_embed, use_bias=in_proj_bias)
    self.out_proj = tf.keras.layers.Dense(d_embed, use_bias=out_proj_bias)
    self.n_heads = n_heads
    self.d_head = d_embed // n_heads

  def call(self, x, causal_mask: bool):
    input_shape = tf.shape(x)
    batch_size, seq_len, d_embed = input_shape

    interim_shape = tf.stack([batch_size, seq_len, self.n_heads, self.d_head])

    q, k, v = tf.split(self.in_proj_bias(x), 3, axis=-1) # Splitting QKV

    # Equivalent to .view(interim_shape).transpose(1, 2)
    q = tf.transpose(tf.reshape(q, interim_shape), perm=[0, 2, 1, 3])
    k = tf.transpose(tf.reshape(k, interim_shape), perm=[0, 2, 1, 3])
    v = tf.transpose(tf.reshape(v, interim_shape), perm=[0, 2, 1, 3])

    # Removed redundant division and corrected math.sqrt to tf.math.sqrt
    weight = tf.matmul(q,k, transpose_b=True) / tf.math.sqrt(tf.cast(self.d_head, tf.float32))

    if causal_mask:
      mask = tf.linalg.band_part(tf.ones_like(weight), 0, -1) - \
           tf.linalg.band_part(tf.ones_like(weight), 0, 0)
      mask = tf.cast(mask, tf.bool)
      weight = tf.where(mask, float('-inf'), weight)

    # Corrected activation function call
    weight = tf.keras.layers.Activation('softmax')(weight)

    output = tf.matmul(weight, v)
    output = tf.reshape(tf.transpose(output, perm=[0, 2, 1, 3]), input_shape)
    #Without the perm =[0,2,1,3], it will change the entire order. This is equivalent to transpose (1,2) in pytorch
    output = self.out_proj(output)
    return output

class CrossAttention(tf.keras.layers.Layer):
  def __init__(self, n_heads: int, d_embed: int, d_cross: int, in_proj_bias = True, out_proj_bias = True):
    super().__init__()
    # Corrected Dense layer initialization: only units argument is needed
    self.q_proj = tf.keras.layers.Dense(d_embed, use_bias=in_proj_bias)
    self.k_proj = tf.keras.layers.Dense(d_embed, use_bias=in_proj_bias)
    self.v_proj = tf.keras.layers.Dense(d_embed, use_bias=in_proj_bias)
    self.out_proj = tf.keras.layers.Dense(d_embed, use_bias=out_proj_bias)
    self.n_heads = n_heads
    self.d_head = d_embed // n_heads

  def call(self, x, y):

    # x (latent): # (Batch_Size, Seq_Len_Q, Dim_Q)
    # y (context): # (Batch_Size, Seq_Len_KV, Dim_KV) = (Batch_Size, 77, 768)

    input_shape = tf.shape(x)
    batch_size, seq_len, d_embed = input_shape

    interim_shape = tf.stack([batch_size, -1, self.n_heads, self.d_head])

    # Projecting the Query, Key and Value for the attention mechanism
    q = self.q_proj(x)
    k = self.k_proj(y)
    v = self.v_proj(y)

    # Equivalent to .view(interim_shape).transpose(1, 2)
    q = tf.transpose(tf.reshape(q, interim_shape), perm=[0, 2, 1, 3])
    k = tf.transpose(tf.reshape(k, interim_shape), perm=[0, 2, 1, 3])
    v = tf.transpose(tf.reshape(v, interim_shape), perm=[0, 2, 1, 3])

    # Removed redundant division and corrected math.sqrt to tf.math.sqrt
    weight = tf.matmul(q,k, transpose_b=True) / tf.math.sqrt(tf.cast(self.d_head, tf.float32))

    # Corrected activation function call
    weight = tf.keras.layers.Activation('softmax')(weight)

    output = tf.matmul(weight, v)
    output = tf.reshape(tf.transpose(output, perm=[0, 2, 1, 3]), input_shape)
    #Without the perm =[0,2,1,3], it will change the entire order. This is equivalent to transpose (1,2) in pytorch
    output = self.out_proj(output)
    return output

# Creating the VAE_Attention and VAE_Residual Block

import tensorflow as tf
import keras
from tensorflow.keras.layers import Input, Dense, Concatenate, Flatten
from tensorflow.keras.models import Model, Sequential

class VAE_AttentionBlock(tf.keras.layers.Layer):
  def __init__(self, channels):
    super().__init__()
    self.channels = channels
    self.groupNorm = keras.layers.GroupNormalization(groups=32, axis =1)
    self.attention = SelfAttention(1, channels)

  def call(self, x):
    # x: (Batch_Size, Features, Height, Width)

    residue = x
    x = self.groupNorm(x)
    #(Batch_size, Features, Height, Width)
    n, c, h, w = x.shape

    x =tf.reshape(x, (n, c, h*w))
    #(Batch_size, Height*Width, Features)

    x = tf.transpose(x, perm=[0, 2, 1])
    #(Batch_size, Features, Height*Width)
    x = self.attention(x, causal_mask=False) # Changed causal_mask to False for VAE Attention

    x = tf.transpose(x, perm=[0, 2, 1])
    #(Batch_size, Height*Width, Features)
    x = tf.reshape(x, (n, c, h, w))

    x= x + residue
    return x

class VAE_ResidualBlock(tf.keras.layers.Layer):
  def __init__(self, in_channels, out_channels):
    super().__init__()
    self.groupNorm1 = keras.layers.GroupNormalization(groups=32, axis =1)
    self.conv1 = tf.keras.layers.Conv2D(out_channels, kernel_size=3, padding='same', data_format='channels_first')

    self.groupNorm2 = keras.layers.GroupNormalization(groups=32, axis =1)
    self.conv2 = tf.keras.layers.Conv2D(out_channels, kernel_size=3, padding='same', data_format='channels_first')

    if in_channels != out_channels:
      self.resual_layer = tf.keras.layers.Conv2D(out_channels, kernel_size=1, padding='valid', data_format='channels_first')
    else:
      self.resual_layer = tf.keras.layers.Activation('linear') # Use 'linear' for no-op activation

  def call(self, x):
    # x: (Batch_Size, In_Channels, Height, Width)

    residue = x

    x = self.groupNorm1(x)
    x = tf.keras.layers.Activation('swish')(x) # Using Activation layer instead of Dense
    x = self.conv1(x)
    x = self.groupNorm2(x)
    x = tf.keras.layers.Activation('swish')(x) # Using Activation layer instead of Dense
    x = self.conv2(x)

    return x + self.resual_layer(residue)

import tensorflow as tf
import keras
from tensorflow.keras.layers import Input, Dense, Concatenate, Flatten, Conv2D, UpSampling2D, Activation
from tensorflow.keras.models import Model, Sequential
import math # Import math for sqrt and exp

class VAE_Decoder(tf.keras.layers.Layer):
  def __init__(self):
    super().__init__()
    self.layers_list = [
        # (Batch_Size, Height/8, Width/8, 4) -> (Batch_Size, Height/8, Width/8, 4)
            Conv2D(4, kernel_size=1, padding='valid'), # padding='valid' for no padding

            # (Batch_Size, Height/8, Width/8, 512)
            Conv2D(512, kernel_size=3, padding='same'),

            VAE_ResidualBlock(512, 512),
            VAE_AttentionBlock(512),
            VAE_ResidualBlock(512, 512),
            VAE_ResidualBlock(512, 512),
            VAE_ResidualBlock(512, 512),
            VAE_ResidualBlock(512, 512),

            # (Batch_Size, Height/4, Width/4, 512)
            UpSampling2D(size=(2, 2)),
            Conv2D(512, kernel_size=3, padding='same'),

            VAE_ResidualBlock(512, 512),
            VAE_ResidualBlock(512, 512),
            VAE_ResidualBlock(512, 512),

            # (Batch_Size, Height/2, Width/2, 512)
            UpSampling2D(size=(2, 2)),
            Conv2D(512, kernel_size=3, padding='same'),

            VAE_ResidualBlock(512, 256),
            VAE_ResidualBlock(256, 256),
            VAE_ResidualBlock(256, 256),

            # (Batch_Size, Height, Width, 256)
            UpSampling2D(size=(2, 2)),
            Conv2D(256, kernel_size=3, padding='same'),

            VAE_ResidualBlock(256, 128),
            VAE_ResidualBlock(128, 128),
            VAE_ResidualBlock(128, 128),

            keras.layers.GroupNormalization(groups=32, axis=1),
            Activation(tf.nn.silu),

            # Final RGB output
            Conv2D(3, kernel_size=3, padding='same')
            ]

  def call(self, x):
    x = x/ 0.18215

    for layer in self.layers_list:
      x = layer(x)
    return x

class VAE_Encoder(tf.keras.layers.Layer):
  def __init__(self):
    super().__init__()
    self.layers_list = [
          Conv2D(128, kernel_size=3, padding='same'),
          VAE_ResidualBlock(128, 128),
          VAE_ResidualBlock(128, 128),
          Conv2D(128, kernel_size=3, strides=2, padding='valid'), # Changed to 'valid' as padding is handled manually
          VAE_ResidualBlock(128, 256),
          VAE_ResidualBlock(256, 256),
          Conv2D(256, kernel_size=3, strides=2, padding='valid'), # Changed to 'valid' as padding is handled manually
          VAE_ResidualBlock(256, 512),
          VAE_ResidualBlock(512, 512),
          Conv2D(512, kernel_size=3, strides=2, padding='valid'), # Changed to 'valid' as padding is handled manually
          VAE_ResidualBlock(512, 512),
          VAE_ResidualBlock(512, 512),
          VAE_ResidualBlock(512, 512),
          VAE_AttentionBlock(512),
          VAE_ResidualBlock(512, 512),
          keras.layers.GroupNormalization(groups=32, axis=1),
          Activation(tf.nn.silu),
          Conv2D(8, kernel_size=3, padding='same'),
          Conv2D(8, kernel_size=1, padding='valid'),
      ]

  def call(self, x, noise):
    # x: (Batch_Size, Channel, Height, Width)
    # noise: (Batch_Size, 4, Height / 8, Width / 8)

    for layer in self.layers_list:
      if isinstance(layer, Conv2D) and layer.strides == (2, 2):
        # Padding during downsampling must be asymmetric
        # padding with zeros on the right and bottom
        x = tf.pad(x, [[0, 0], [0, 0], [1, 0], [1, 0]]) # Assuming NCHW format based on previous uses, if NHWC, then [0,0], [1,0], [1,0], [0,0]
      x = layer(x)

    mean, logvar = tf.split(x, num_or_size_splits=2, axis=1)

    logvar = tf.clip_by_value(logvar, -30.0, 20.0)  # Clamp the log variance between -30 and 20, so that the variance is between (circa) 1e-14 and 1e8.

    variance = tf.exp(logvar)
    stdev = tf.sqrt(variance)

    x = mean + stdev * noise

    x =x * 0.18215 ## Scaling by a constant variable

    return x

# Creating the Clip Encoder

import tensorflow as tf
import keras
from tensorflow.keras.layers import Input, Dense, Concatenate, Flatten
from tensorflow.keras.models import Model, Sequential

class CLIPEmbedding(tf.keras.layers.Layer):
  def __init__(self, n_vocab:int, n_embd: int, n_token: int):
    super().__init__()
    self.token_embedding = tf.keras.layers.Embedding(n_vocab, n_embd)
    # Corrected tf.zeros shape argument
    self.position_embedding = tf.Variable(tf.zeros((n_token, n_embd)))

  def call(self, tokens):
    x = self.token_embedding(tokens)
    x = x + self.position_embedding
    return x

class CLIPLayer(tf.keras.layers.Layer):
  def __init__(self, n_head: int, n_embd: int):
    super().__init__()
    self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-5) # Added epsilon as is common for LayerNorm
    self.attn = SelfAttention(n_head, n_embd)
    self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-5)
    self.linear1 = tf.keras.layers.Dense(4*n_embd, use_bias=True)
    self.linear2 = tf.keras.layers.Dense(n_embd, use_bias=True)

  def call(self, x):
    residue = x

    # Self Attention
    x = self.layer_norm1(x)
    x= self.attn(x, causal_mask=True)
    x = x + residue

    # Feed Forward
    # Applying the feed forward where the forward dimension is 4 times the embedding dimension

    residue = x
    x = self.layer_norm2(x)
    x = self.linear1(x)
    x = tf.nn.gelu(x) # Taking a quick GELU function
    x = self.linear2(x)
    x = x + residue
    return x # Added return statement for the layer

class CLIP(tf.keras.layers.Layer):
  def __init__(self):
    super().__init__()
    self.embedding = CLIPEmbedding(n_vocab=49408, n_embd=768, n_token=77)
    self.layers = [CLIPLayer(n_head=12, n_embd=768) for _ in range(12)]
    self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5) # Added epsilon

  def call (self, tokens):
    # Removed .type(torch.long) as it's PyTorch specific
    x = self.embedding(tokens)
    for layer in self.layers:
      x = layer(x)
    x = self.layer_norm(x)
    return x

# Creating the Unet

import tensorflow as tf
import keras
from tensorflow.keras.layers import Input, Dense, Concatenate, Flatten, Conv2D, UpSampling2D, Activation
from tensorflow.keras.models import Model, Sequential

class TimeEmbedding(tf.keras.layers.Layer):
  def __init__(self, n_embd: int):
    super().__init__()
    self.linear1 = Dense(4*n_embd, use_bias=True)
    self.linear2 = Dense(4*n_embd, use_bias=True)

  def call (self, x):
    # x: (1, 320)

    # (1, 1280) -> (1, 1280)
    x = self.linear1(x)

    # (1, 1280) -> (1, 1280)
    x = tf.nn.gelu(x)

    # (1, 1280) -> (1, 1280)
    x = self.linear2(x)

    return x


class UNET_ResidualBlock(tf.keras.layers.Layer):
  def __init__(self, in_channels: int, out_channels: int, n_time = 1280):
    super().__init__()
    self.groupnormfeature = keras.layers.GroupNormalization(groups=32, axis=1)
    self.conv_feature = Conv2D(out_channels, kernel_size=3, padding=1)
    self.linear_time = Dense(out_channels, use_bias=True)
    self.groupnormmerged = keras.layers.GroupNormalization(groups=32, axis=1)
    self.conv_merged = Conv2D(out_channels, kernel_size=3, padding=1, data_format='channels_first')

    if in_channels != out_channels:
      self.residual_layer = Conv2D(out_channels, kernel_size=1, padding='valid', data_format='channels_first')
    else:
      self.residual_layer = Activation('linear') # Use 'linear' for no-op activation

  def call (self, feature, time):
    # feature: (Batch_Size, In_Channels, Height, Width)
    # time: (1, 1280)

    residue = feature

    feature = self.groupnormfeature(feature)
    feature = Activation('swish')(feature)
    feature = self.conv_feature(feature)

    time = Activation('swish')(time)
    time = self.linear_time(time)

    # Add width and height dimension to time.
    # (Batch_Size, Out_Channels, Height, Width) + (1, Out_Channels, 1, 1) -> (Batch_Size, Out_Channels, Height, Width)
    # Assuming NHWC format (Batch, Height, Width, Channels)
    merged = feature + tf.expand_dims(tf.expand_dims(time, axis=1), axis=1) # Equivalent to .unsqueeze(-1).unsqueeze(-1)

    merged = self.groupnormmerged(merged)

    merged = Activation('swish')(merged)
    merged = self.conv_merged(merged)

    return merged + self.residual_layer(residue)

class UNET_AttentionBlock(tf.keras.layers.Layer):
  def __init__(self, n_head:int, n_embd:int, d_context = 768):
    super().__init__()
    channels = n_head * n_embd

    self.groupnorm = keras.layers.GroupNormalization(groups=32, axis=1)
    self.conv_input = Conv2D(channels, kernel_size=1, padding='valid', data_format='channels_first')
    self.layernorm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-5) # Added epsilon as it's common for LayerNorm
    self.attention1 = SelfAttention(n_head, channels, in_proj_bias=False)
    self.layernorm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-5)
    self.attention2 = CrossAttention(n_head, channels, d_context, in_proj_bias=False)
    self.layernorm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-5)
    self.linear_geglu_1 = Dense(4*channels, use_bias=True)
    self.linear_geglu_2 = Dense(channels, use_bias=True)

    self.conv_output = Conv2D(channels, kernel_size=1, padding= 'valid', data_format='channels_first')

  def call(self, x, context):
    # x: (Batch_size, Features, Height, Width) -> assuming NCHW (Batch, Channels, Height, Width)
    # context: (Batch_size, Sequence_length, Dim)

    residue_long = x

    x = self.groupnorm(x)
    x = self.conv_input(x)
    n, c_orig, h_orig, w_orig = x.shape # Assuming NCHW
    # Batch_size, height, width, features

    x = tf.reshape(x, (n, c_orig, h_orig*w_orig))
    x = tf.transpose(x, perm=[0, 2, 1]) # (Batch_size, Height*Width, Channels)

    residue_short = x
    x = self.layernorm_1(x)
    x = self.attention1(x, causal_mask=False) # Causal mask set to False for UNET attention

    x = x + residue_short
    residue_short = x

    x = self.layernorm_2(x)
    x = self.attention2(x, context)

    x = x + residue_short
    residue_short = x

    # Normalization + FFN with GeGLU and skip connection

    # (Batch_Size, Height * Width, Features) -> (Batch_Size, Height * Width, Features)
    x = self.layernorm_3(x)

    # GeGLU as implemented in the original code: https://github.com/CompVis/stable-diffusion/blob/21f890f9da3cfbeaba8e2ac3c425ee9e998d5229/ldm/modules/attention.py#L37C10-L37C10
    # (Batch_Size, Height * Width, Features) -> two tensors of shape (Batch_Size, Height * Width, Features * 4)

    x = self.linear_geglu_1(x)

    x, gate = tf.split(x, num_or_size_splits=2, axis=-1);
    x = x * tf.nn.gelu(gate)

    x = self.linear_geglu_2(x)

    x = x + residue_short

    x = tf.transpose(x, perm=[0, 2, 1]) # (Batch_size, Channels, Height*Width)
    x = tf.reshape(x, (n, c_orig, h_orig, w_orig))
    # Batch_size, features, height, width

    # Final skip connection between initial input and output of the block
    # (Batch_Size, Features, Height, Width) + (Batch_Size, Features, Height, Width) -> (Batch_Size, Features, Height, Width)
    return self.conv_output(x) + residue_long

import tensorflow as tf
import keras
from tensorflow.keras.layers import Input, Dense, Concatenate, Flatten, Conv2D, UpSampling2D, Activation
from tensorflow.keras.models import Model, Sequential

class Upsample(tf.keras.layers.Layer):
  def __init__(self, channels):
    super().__init__()
    self.conv = Conv2D(channels, kernel_size=3, padding='same')

  def call(self,x):
    # (Batch_Size, Features, Height, Width) -> (Batch_Size, Features, Height * 2, Width * 2)
    x = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation = 'nearest')(x)
    return self.conv(x)

class SwitchSequential(tf.keras.layers.Layer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.layers_list = args # Store layers as a list

    def call(self, x, context, time):
        for layer in self.layers_list:
            if isinstance(layer, UNET_AttentionBlock):
                x = layer(x, context)
            elif isinstance(layer, UNET_ResidualBlock):
                x = layer(x, time)
            else:
                x = layer(x)
        return x

class UNET(tf.keras.layers.Layer):
  def __init__(self):
    super().__init__()
    self.encoders = [
        # (Batch_Size, 4, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8)
        SwitchSequential(tf.keras.layers.Conv2D(320, kernel_size=3, padding='same')),

        # (Batch_Size, 320, Height / 8, Width / 8) -> # (Batch_Size, 320, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8)
        SwitchSequential(UNET_ResidualBlock(320, 320), UNET_AttentionBlock(8, 40)),

        # (Batch_Size, 320, Height / 8, Width / 8) -> # (Batch_Size, 320, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8)
        SwitchSequential(UNET_ResidualBlock(320, 320), UNET_AttentionBlock(8, 40)),

        # (Batch_Size, 320, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 16, Width / 16)
        SwitchSequential(tf.keras.layers.Conv2D(320, kernel_size=3, strides=2, padding='same')),

        # (Batch_Size, 320, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 16, Width / 16)
        SwitchSequential(UNET_ResidualBlock(320, 640), UNET_AttentionBlock(8, 80)),

        # (Batch_Size, 640, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 16, Width / 16)
        SwitchSequential(UNET_ResidualBlock(640, 640), UNET_AttentionBlock(8, 80)),

        # (Batch_Size, 640, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 32, Width / 32)
            SwitchSequential(tf.keras.layers.Conv2D(640, 640, kernel_size=3, strides=2, padding='same')),

        # (Batch_Size, 640, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 32, Width / 32)
        SwitchSequential(UNET_ResidualBlock(640, 1280), UNET_AttentionBlock(8, 160)),

        # (Batch_Size, 1280, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 32, Width / 32)
        SwitchSequential(UNET_ResidualBlock(1280, 1280), UNET_AttentionBlock(8, 160)),

        # (Batch_Size, 1280, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 64, Width / 64)
        SwitchSequential(tf.keras.layers.Conv2D(1280, 1280, kernel_size=3, strides=2, padding='same')),

        # (Batch_Size, 1280, Height / 64, Width / 64) -> (Batch_Size, 1280, Height / 64, Width / 64)
        SwitchSequential(UNET_ResidualBlock(1280, 1280)),

        # (Batch_Size, 1280, Height / 64, Width / 64) -> (Batch_Size, 1280, Height / 64, Width / 64)
        SwitchSequential(UNET_ResidualBlock(1280, 1280)),
    ]

    self.bottleneck = SwitchSequential(
        # (Batch_Size, 1280, Height / 64, Width / 64) -> (Batch_Size, 1280, Height / 64, Width / 64)
        UNET_ResidualBlock(1280, 1280),

        # (Batch_Size, 1280, Height / 64, Width / 64) -> (Batch_Size, 1280, Height / 64, Width / 64)
        UNET_AttentionBlock(8, 160),

        # (Batch_Size, 1280, Height / 64, Width / 64) -> (Batch_Size, 1280, Height / 64, Width / 64)
        UNET_ResidualBlock(1280, 1280),
    )

    self.decoders = [
        # (Batch_Size, 2560, Height / 64, Width / 64) -> (Batch_Size, 1280, Height / 64, Width / 64)
        SwitchSequential(UNET_ResidualBlock(2560, 1280)),

        # (Batch_Size, 2560, Height / 64, Width / 64) -> (Batch_Size, 1280, Height / 64, Width / 64)
        SwitchSequential(UNET_ResidualBlock(2560, 1280)),

        # (Batch_Size, 2560, Height / 64, Width / 64) -> (Batch_Size, 1280, Height / 64, Width / 64) -> (Batch_Size, 1280, Height / 32, Width / 32)
        SwitchSequential(UNET_ResidualBlock(2560, 1280), Upsample(1280)),

        # (Batch_Size, 2560, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 32, Width / 32)
        SwitchSequential(UNET_ResidualBlock(2560, 1280), UNET_AttentionBlock(8, 160)),

        # (Batch_Size, 2560, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 32, Width / 32)
        SwitchSequential(UNET_ResidualBlock(2560, 1280), UNET_AttentionBlock(8, 160)),

        # (Batch_Size, 1920, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 16, Width / 16)
        SwitchSequential(UNET_ResidualBlock(1920, 1280), UNET_AttentionBlock(8, 160), Upsample(1280)),

        # (Batch_Size, 1920, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 16, Width / 16)
        SwitchSequential(UNET_ResidualBlock(1920, 640), UNET_AttentionBlock(8, 80)),

        # (Batch_Size, 1280, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 16, Width / 16)
        SwitchSequential(UNET_ResidualBlock(1280, 640), UNET_AttentionBlock(8, 80)),

        # (Batch_Size, 960, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 8, Width / 8)
        SwitchSequential(UNET_ResidualBlock(960, 640), UNET_AttentionBlock(8, 80), Upsample(640)),

        # (Batch_Size, 960, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8)
        SwitchSequential(UNET_ResidualBlock(960, 320), UNET_AttentionBlock(8, 40)),

        # (Batch_Size, 640, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8)
        SwitchSequential(UNET_ResidualBlock(640, 320), UNET_AttentionBlock(8, 40)),

        # (Batch_Size, 640, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8)
        SwitchSequential(UNET_ResidualBlock(640, 320), UNET_AttentionBlock(8, 40)),
    ]

  def call (self, x, context, time):
    # x: (Batch_Size, 4, Height / 8, Width / 8)
    # context: (Batch_Size, Seq_Len, Dim)
    # time: (1, 1280)

    skip_connections = []
    for layers in self.encoders:
      x = layers(x, context, time)
      skip_connections.append(x)

    x = self.bottleneck(x, context, time)

    for i, layers in enumerate(self.decoders):
    # Since we always concat with the skip connection of the encoder, the number of features increases before being sent to the decoder's layer
      x = tf.concat((x, skip_connections.pop()), axis=1)
      x = layers(x, context, time)

    return x

import tensorflow as tf
import keras
from tensorflow.keras.layers import Input, Dense, Concatenate, Flatten, Conv2D
from tensorflow.keras.models import Model, Sequential

class UNET_OutputLayers(tf.keras.layers.Layer):
  def __init__(self, in_channels: int, out_channels: int):
    super().__init__()
    self.groupnorm =  tf.keras.layers.GroupNormalization(groups=32, axis=1)
    self.conv = Conv2D(out_channels, kernel_size=3, padding='same')

  def call (self, x):
    # x: (Batch_Size, 320, Height / 8, Width / 8)

    # (Batch_Size, 320, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8)
    x = self.groupnorm(x)

    # (Batch_Size, 320, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8)
    x = tf.keras.layers.Activation('swish')(x)

    # (Batch_Size, 320, Height / 8, Width / 8) -> (Batch_Size, 4, Height / 8, Width / 8)
    x = self.conv(x)

    # (Batch_Size, 4, Height / 8, Width / 8)
    return x

class Diffusion(tf.keras.layers.Layer):
    def __init__(self):
        super().__init__()
        self.time_embedding = TimeEmbedding(320)
        self.unet = UNET()
        self.final = UNET_OutputLayers(320, 4)

    def call(self, latent, context, time):
        # latent: (Batch_Size, 4, Height / 8, Width / 8)
        # context: (Batch_Size, Seq_Len, Dim)
        # time: (1, 320)

        # (1, 320) -> (1, 1280)
        time = self.time_embedding(time)

        # (Batch, 4, Height / 8, Width / 8) -> (Batch, 320, Height / 8, Width / 8)
        output = self.unet(latent, context, time)

        # (Batch, 320, Height / 8, Width / 8) -> (Batch, 4, Height / 8, Width / 8)
        output = self.final(output)

        # (Batch, 4, Height / 8, Width / 8)
        return output

# Creating the other parameters like the noise generator etc

import tensorflow as tf
import keras
from tensorflow.keras.layers import Input, Dense, Concatenate, Flatten
from tensorflow.keras.models import Model, Sequential

class DDPMSampler:
  def __init__(self, generator, num_training_steps = 1000, beta_start:float = 0.00085, beta_end = 0.0120):
    # Params "beta_start" and "beta_end" taken from: https://github.com/CompVis/stable-diffusion/blob/21f890f9da3cfbeaba8e2ac3c425ee9e998d5229/configs/stable-diffusion/v1-inference.yaml#L5C8-L5C8
    # For the naming conventions, refer to the DDPM paper (https://arxiv.org/pdf/2006.11239.pdf)
    self.betas = tf.linspace(
    tf.pow(beta_start, 0.5),
    tf.pow(beta_end, 0.5),
    num_training_steps
    )
    self.betas = tf.cast(self.betas, dtype=tf.float32) ** 2

    self.alphas = 1.0 - self.betas
    self.alphas_cumprod = tf.math.cumprod(self.alphas, axis=0) # Cumprod = cummulative product, essential for creating Markov chains in diffusion models
    self.one = tf.constant(1.0, dtype=tf.float32)

    self.generator = generator
    self.num_train_timesteps = num_training_steps

    # Equivalent to np.arange(0, num_training_steps)[::-1]
    # Creates a tensor: [999, 998, ..., 0]
    self.timesteps = tf.range(start=num_training_steps - 1, limit=-1, delta=-1, dtype=tf.int64)

  def set_inference_timesteps(self, num_inference_steps=50):
      self.num_inference_steps = num_inference_steps
      step_ratio = self.num_train_timesteps // self.num_inference_steps
      # Equivalent to (np.arange(0, num_inference_steps) * step_ratio).round()[::-1].copy().astype(np.int64)
      timesteps = tf.cast(tf.round(tf.cast(tf.range(0, num_inference_steps), tf.float32) * tf.cast(step_ratio, tf.float32)), tf.int64)
      self.timesteps = tf.reverse(timesteps, axis=[0])

  def _get_previous_timestep(self, timestep: int) -> int:
      prev_t = timestep - self.num_train_timesteps // self.num_inference_steps
      return prev_t

  def _get_variance(self, timestep: int) -> tf.Tensor:

      prev_t = self._get_previous_timestep(timestep)
      alpha_prod_t = self.alphas_cumprod[timestep]
      alpha_prod_t_prev = self.alphas_cumprod[prev_t] if prev_t >= 0 else self.one
      current_beta_t = 1 - alpha_prod_t / alpha_prod_t_prev

      # For t > 0, compute predicted variance βt (see formula (6) and (7) from https://arxiv.org/pdf/2006.11239.pdf)
      # and sample from it to get previous sample
      # x_{t-1} ~ N(pred_prev_sample, variance) == add variance to pred_sample

      variance = (1 - alpha_prod_t_prev) / (1 - alpha_prod_t) * current_beta_t

      variance = tf.clip_by_value(variance, 1e-20, 1)
      return variance

  def set_strength(self, strength = 1):
      """
          Set how much noise to add to the input image.
          More noise (strength ~ 1) means that the output will be further from the input image.
          Less noise (strength ~ 0) means that the output will be closer to the input image.
      """
      # start_step is the number of noise levels to skip
      start_step = self.num_inference_steps - int(self.num_inference_steps * strength)
      self.timesteps = self.timesteps[start_step:]
      self.start_step = start_step

  def step(self, timestep: int, latents: tf.Tensor, model_output: tf.Tensor):
      t = timestep
      prev_t = self._get_previous_timestep(t)

      # 1. compute alphas, betas
      alpha_prod_t = self.alphas_cumprod[t]
      alpha_prod_t_prev = self.alphas_cumprod[prev_t] if prev_t >= 0 else self.one
      beta_prod_t = 1 - alpha_prod_t
      beta_prod_t_prev = 1 - alpha_prod_t_prev
      current_alpha_t = alpha_prod_t / alpha_prod_t_prev
      current_beta_t = 1 - current_alpha_t

      # 2. compute predicted original sample from predicted noise also called
      # "predicted x_0" of formula (15) from https://arxiv.org/pdf/2006.11239.pdf
      pred_original_sample = (latents - beta_prod_t ** (0.5) * model_output) / alpha_prod_t ** (0.5)

      # 4. Compute coefficients for pred_original_sample x_0 and current sample x_t
      # See formula (7) from https://arxiv.org/pdf/2006.11239.pdf
      pred_original_sample_coeff = (alpha_prod_t_prev ** (0.5) * current_beta_t) / beta_prod_t
      current_sample_coeff = current_alpha_t ** (0.5) * beta_prod_t_prev / beta_prod_t

      # 5. Compute predicted previous sample µ_t
      # See formula (7) from https://arxiv.org/pdf/2006.11239.pdf
      pred_prev_sample = pred_original_sample_coeff * pred_original_sample + current_sample_coeff * latents

      # 6. Add noise
      variance = 0
      if t > 0:
          # Removed tf.device(model_output.device) as .device is PyTorch specific.
          # tf.random.normal will typically run on the default device or the device of other tensors if they are used in its context.
          noise = tf.random.normal(
            shape=tf.shape(model_output),
            dtype=model_output.dtype
          )
          # Compute the variance as per formula (7) from https://arxiv.org/pdf/2006.11239.pdf
          variance = (self._get_variance(t) ** 0.5) * noise

      # sample from N(mu, sigma) = X can be obtained by X = mu + sigma * N(0, 1)
      # the variable "variance" is already multiplied by the noise N(0, 1)
      pred_prev_sample = pred_prev_sample + variance

      return pred_prev_sample

  def add_noise(
        self,
        original_samples: tf.Tensor, # Changed from tf.FloatTensor
        timesteps: tf.Tensor,       # Changed from tf.IntTensor
    ) -> tf.Tensor:                 # Changed from tf.FloatTensor
        alphas_cumprod = self.alphas_cumprod
        alphas_cumprod_at_timesteps = tf.gather(alphas_cumprod, timesteps)

        sqrt_alpha_prod = alphas_cumprod_at_timesteps ** 0.5
        sqrt_alpha_prod = tf.reshape(sqrt_alpha_prod, [-1]) # Equivalent to flatten()
        while len(sqrt_alpha_prod.shape) < len(original_samples.shape):
            sqrt_alpha_prod = tf.expand_dims(sqrt_alpha_prod, axis=-1) # Equivalent to unsqueeze(-1)

        sqrt_one_minus_alpha_prod = (1 - alphas_cumprod_at_timesteps) ** 0.5
        sqrt_one_minus_alpha_prod = tf.reshape(sqrt_one_minus_alpha_prod, [-1]) # Equivalent to flatten()
        while len(sqrt_one_minus_alpha_prod.shape) < len(original_samples.shape):
            sqrt_one_minus_alpha_prod = tf.expand_dims(sqrt_one_minus_alpha_prod, axis=-1) # Equivalent to unsqueeze(-1)

        # Sample from q(x_t | x_0) as in equation (4) of https://arxiv.org/pdf/2006.11239.pdf
        # Because N(mu, sigma) = X can be obtained by X = mu + sigma * N(0, 1)
        # here mu = sqrt_alpha_prod * original_samples and sigma = sqrt_one_minus_alpha_prod

        noise = tf.random.normal(
          shape=tf.shape(original_samples),
          dtype=original_samples.dtype
        )

        noisy_samples = sqrt_alpha_prod * original_samples + sqrt_one_minus_alpha_prod * noise
        return noisy_samples

from huggingface_hub import hf_hub_download

repo_id = "runwayml/stable-diffusion-v1-5" # Model repository ID
filename = "v1-5-pruned-emaonly.ckpt"      # Specific file name in the repo

# Download the file
cache_path = hf_hub_download(
    repo_id=repo_id,
    filename=filename,
    cache_dir="/content/mydownloadedweights" # Optional: specify a local directory
)

import torch
import tensorflow as tf

def load_from_standard_weights(input_file: str, device: str) -> dict[str, dict[str, tf.Tensor]]:
    # Taken from: https://github.com/kjsman/stable-diffusion-pytorch/issues/7#issuecomment-1426839447
    original_model = torch.load(input_file, map_location=device, weights_only = False)["state_dict"]

    converted = {}
    converted['diffusion'] = {}
    converted['encoder'] = {}
    converted['decoder'] = {}
    converted['clip'] = {}

    # Helper function to convert and transpose PyTorch tensors to TensorFlow
    def to_tf_tensor(param, permute_conv=False, permute_linear=False):
        if isinstance(param, torch.Tensor):
            tf_tensor = tf.convert_to_tensor(param.cpu().numpy(), dtype=tf.float32)
            if permute_conv and len(tf_tensor.shape) == 4: # Conv2D weights
                return tf.transpose(tf_tensor, perm=[2, 3, 1, 0]) # PyTorch: out, in, h, w -> TF: h, w, in, out
            elif permute_linear and len(tf_tensor.shape) == 2: # Linear weights
                return tf.transpose(tf_tensor, perm=[1, 0]) # PyTorch: out, in -> TF: in, out
            return tf_tensor
        return param

    # Diffusion Model Weights
    converted['diffusion']['time_embedding.linear_1.weight'] = to_tf_tensor(original_model['model.diffusion_model.time_embed.0.weight'], permute_linear=True)
    converted['diffusion']['time_embedding.linear_1.bias'] = to_tf_tensor(original_model['model.diffusion_model.time_embed.0.bias'])
    converted['diffusion']['time_embedding.linear_2.weight'] = to_tf_tensor(original_model['model.diffusion_model.time_embed.2.weight'], permute_linear=True)
    converted['diffusion']['time_embedding.linear_2.bias'] = to_tf_tensor(original_model['model.diffusion_model.time_embed.2.bias'])

    converted['diffusion']['unet.encoders.0.0.weight'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.0.0.weight'], permute_conv=True)
    converted['diffusion']['unet.encoders.0.0.bias'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.0.0.bias'])

    # Encoder blocks (Residual and Attention)
    for i in [1, 2, 4, 5, 7, 8, 10, 11]: # Corresponding to input_blocks 1,2,4,5,7,8,10,11
        # Residual Block parts
        converted['diffusion']['unet.encoders.' + str(i) + '.0.groupnorm_feature.weight'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.0.in_layers.0.weight'])
        converted['diffusion']['unet.encoders.' + str(i) + '.0.groupnorm_feature.bias'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.0.in_layers.0.bias'])
        converted['diffusion']['unet.encoders.' + str(i) + '.0.conv_feature.weight'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.0.in_layers.2.weight'], permute_conv=True)
        converted['diffusion']['unet.encoders.' + str(i) + '.0.conv_feature.bias'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.0.in_layers.2.bias'])
        converted['diffusion']['unet.encoders.' + str(i) + '.0.linear_time.weight'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.0.emb_layers.1.weight'], permute_linear=True)
        converted['diffusion']['unet.encoders.' + str(i) + '.0.linear_time.bias'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.0.emb_layers.1.bias'])
        converted['diffusion']['unet.encoders.' + str(i) + '.0.groupnorm_merged.weight'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.0.out_layers.0.weight'])
        converted['diffusion']['unet.encoders.' + str(i) + '.0.groupnorm_merged.bias'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.0.out_layers.0.bias'])
        converted['diffusion']['unet.encoders.' + str(i) + '.0.conv_merged.weight'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.0.out_layers.3.weight'], permute_conv=True)
        converted['diffusion']['unet.encoders.' + str(i) + '.0.conv_merged.bias'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.0.out_layers.3.bias'])
        if i in [4, 7]: # These blocks have a residual layer shortcut
            converted['diffusion']['unet.encoders.' + str(i) + '.0.residual_layer.weight'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.0.skip_connection.weight'], permute_conv=True)
            converted['diffusion']['unet.encoders.' + str(i) + '.0.residual_layer.bias'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.0.skip_connection.bias'])

        # Attention Block parts (if applicable)
        if i not in [10, 11]: # Blocks 1,2,4,5,7,8 have attention
            converted['diffusion']['unet.encoders.' + str(i) + '.1.groupnorm.weight'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.1.norm.weight'])
            converted['diffusion']['unet.encoders.' + str(i) + '.1.groupnorm.bias'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.1.norm.bias'])
            converted['diffusion']['unet.encoders.' + str(i) + '.1.conv_input.weight'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.1.proj_in.weight'], permute_conv=True)
            converted['diffusion']['unet.encoders.' + str(i) + '.1.conv_input.bias'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.1.proj_in.bias'])
            # In_proj weights for QKV are concatenated from 3 separate weights
            qkv_weight = torch.cat((original_model['model.diffusion_model.input_blocks.' + str(i) + '.1.transformer_blocks.0.attn1.to_q.weight'],
                                   original_model['model.diffusion_model.input_blocks.' + str(i) + '.1.transformer_blocks.0.attn1.to_k.weight'],
                                   original_model['model.diffusion_model.input_blocks.' + str(i) + '.1.transformer_blocks.0.attn1.to_v.weight']), 0)
            converted['diffusion']['unet.encoders.' + str(i) + '.1.attention_1.in_proj.weight'] = to_tf_tensor(qkv_weight, permute_linear=True)
            converted['diffusion']['unet.encoders.' + str(i) + '.1.attention_1.out_proj.weight'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.1.transformer_blocks.0.attn1.to_out.0.weight'], permute_linear=True)
            converted['diffusion']['unet.encoders.' + str(i) + '.1.attention_1.out_proj.bias'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.1.transformer_blocks.0.attn1.to_out.0.bias'])
            converted['diffusion']['unet.encoders.' + str(i) + '.1.linear_geglu_1.weight'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.1.transformer_blocks.0.ff.net.0.proj.weight'], permute_linear=True)
            converted['diffusion']['unet.encoders.' + str(i) + '.1.linear_geglu_1.bias'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.1.transformer_blocks.0.ff.net.0.proj.bias'])
            converted['diffusion']['unet.encoders.' + str(i) + '.1.linear_geglu_2.weight'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.1.transformer_blocks.0.ff.net.2.weight'], permute_linear=True)
            converted['diffusion']['unet.encoders.' + str(i) + '.1.linear_geglu_2.bias'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.1.transformer_blocks.0.ff.net.2.bias'])
            converted['diffusion']['unet.encoders.' + str(i) + '.1.attention_2.q_proj.weight'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.1.transformer_blocks.0.attn2.to_q.weight'], permute_linear=True)
            converted['diffusion']['unet.encoders.' + str(i) + '.1.attention_2.k_proj.weight'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.1.transformer_blocks.0.attn2.to_k.weight'], permute_linear=True)
            converted['diffusion']['unet.encoders.' + str(i) + '.1.attention_2.v_proj.weight'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.1.transformer_blocks.0.attn2.to_v.weight'], permute_linear=True)
            converted['diffusion']['unet.encoders.' + str(i) + '.1.attention_2.out_proj.weight'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.1.transformer_blocks.0.attn2.to_out.0.weight'], permute_linear=True)
            converted['diffusion']['unet.encoders.' + str(i) + '.1.attention_2.out_proj.bias'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.1.transformer_blocks.0.attn2.to_out.0.bias'])
            converted['diffusion']['unet.encoders.' + str(i) + '.1.layernorm_1.weight'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.1.transformer_blocks.0.norm1.weight'])
            converted['diffusion']['unet.encoders.' + str(i) + '.1.layernorm_1.bias'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.1.transformer_blocks.0.norm1.bias'])
            converted['diffusion']['unet.encoders.' + str(i) + '.1.layernorm_2.weight'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.1.transformer_blocks.0.norm2.weight'])
            converted['diffusion']['unet.encoders.' + str(i) + '.1.layernorm_2.bias'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.1.transformer_blocks.0.norm2.bias'])
            converted['diffusion']['unet.encoders.' + str(i) + '.1.layernorm_3.weight'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.1.transformer_blocks.0.norm3.weight'])
            converted['diffusion']['unet.encoders.' + str(i) + '.1.layernorm_3.bias'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.1.transformer_blocks.0.norm3.bias'])
            converted['diffusion']['unet.encoders.' + str(i) + '.1.conv_output.weight'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.1.proj_out.weight'], permute_conv=True)
            converted['diffusion']['unet.encoders.' + str(i) + '.1.conv_output.bias'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.1.proj_out.bias'])

    # Downsample layers
    for i in [3, 6, 9]:
        converted['diffusion']['unet.encoders.' + str(i) + '.0.weight'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.0.op.weight'], permute_conv=True)
        converted['diffusion']['unet.encoders.' + str(i) + '.0.bias'] = to_tf_tensor(original_model['model.diffusion_model.input_blocks.' + str(i) + '.0.op.bias'])


    # Bottleneck layers
    converted['diffusion']['unet.bottleneck.0.groupnorm_feature.weight'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.0.in_layers.0.weight'])
    converted['diffusion']['unet.bottleneck.0.groupnorm_feature.bias'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.0.in_layers.0.bias'])
    converted['diffusion']['unet.bottleneck.0.conv_feature.weight'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.0.in_layers.2.weight'], permute_conv=True)
    converted['diffusion']['unet.bottleneck.0.conv_feature.bias'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.0.in_layers.2.bias'])
    converted['diffusion']['unet.bottleneck.0.linear_time.weight'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.0.emb_layers.1.weight'], permute_linear=True)
    converted['diffusion']['unet.bottleneck.0.linear_time.bias'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.0.emb_layers.1.bias'])
    converted['diffusion']['unet.bottleneck.0.groupnorm_merged.weight'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.0.out_layers.0.weight'])
    converted['diffusion']['unet.bottleneck.0.groupnorm_merged.bias'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.0.out_layers.0.bias'])
    converted['diffusion']['unet.bottleneck.0.conv_merged.weight'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.0.out_layers.3.weight'], permute_conv=True)
    converted['diffusion']['unet.bottleneck.0.conv_merged.bias'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.0.out_layers.3.bias'])

    converted['diffusion']['unet.bottleneck.1.groupnorm.weight'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.1.norm.weight'])
    converted['diffusion']['unet.bottleneck.1.groupnorm.bias'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.1.norm.bias'])
    converted['diffusion']['unet.bottleneck.1.conv_input.weight'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.1.proj_in.weight'], permute_conv=True)
    converted['diffusion']['unet.bottleneck.1.conv_input.bias'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.1.proj_in.bias'])
    qkv_weight = torch.cat((original_model['model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight'],
                           original_model['model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight'],
                           original_model['model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight']), 0)
    converted['diffusion']['unet.bottleneck.1.attention_1.in_proj.weight'] = to_tf_tensor(qkv_weight, permute_linear=True)
    converted['diffusion']['unet.bottleneck.1.attention_1.out_proj.weight'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight'], permute_linear=True)
    converted['diffusion']['unet.bottleneck.1.attention_1.out_proj.bias'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias'])
    converted['diffusion']['unet.bottleneck.1.linear_geglu_1.weight'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight'], permute_linear=True)
    converted['diffusion']['unet.bottleneck.1.linear_geglu_1.bias'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias'])
    converted['diffusion']['unet.bottleneck.1.linear_geglu_2.weight'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight'], permute_linear=True)
    converted['diffusion']['unet.bottleneck.1.linear_geglu_2.bias'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias'])
    converted['diffusion']['unet.bottleneck.1.attention_2.q_proj.weight'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight'], permute_linear=True)
    converted['diffusion']['unet.bottleneck.1.attention_2.k_proj.weight'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight'], permute_linear=True)
    converted['diffusion']['unet.bottleneck.1.attention_2.v_proj.weight'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight'], permute_linear=True)
    converted['diffusion']['unet.bottleneck.1.attention_2.out_proj.weight'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight'], permute_linear=True)
    converted['diffusion']['unet.bottleneck.1.attention_2.out_proj.bias'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias'])
    converted['diffusion']['unet.bottleneck.1.layernorm_1.weight'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight'])
    converted['diffusion']['unet.bottleneck.1.layernorm_1.bias'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias'])
    converted['diffusion']['unet.bottleneck.1.layernorm_2.weight'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight'])
    converted['diffusion']['unet.bottleneck.1.layernorm_2.bias'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.bias'])
    converted['diffusion']['unet.bottleneck.1.layernorm_3.weight'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight'])
    converted['diffusion']['unet.bottleneck.1.layernorm_3.bias'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias'])
    converted['diffusion']['unet.bottleneck.1.conv_output.weight'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.1.proj_out.weight'], permute_conv=True)
    converted['diffusion']['unet.bottleneck.1.conv_output.bias'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.1.proj_out.bias'])

    converted['diffusion']['unet.bottleneck.2.groupnorm_feature.weight'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.2.in_layers.0.weight'])
    converted['diffusion']['unet.bottleneck.2.groupnorm_feature.bias'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.2.in_layers.0.bias'])
    converted['diffusion']['unet.bottleneck.2.conv_feature.weight'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.2.in_layers.2.weight'], permute_conv=True)
    converted['diffusion']['unet.bottleneck.2.conv_feature.bias'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.2.in_layers.2.bias'])
    converted['diffusion']['unet.bottleneck.2.linear_time.weight'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.2.emb_layers.1.weight'], permute_linear=True)
    converted['diffusion']['unet.bottleneck.2.linear_time.bias'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.2.emb_layers.1.bias'])
    converted['diffusion']['unet.bottleneck.2.groupnorm_merged.weight'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.2.out_layers.0.weight'])
    converted['diffusion']['unet.bottleneck.2.groupnorm_merged.bias'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.2.out_layers.0.bias'])
    converted['diffusion']['unet.bottleneck.2.conv_merged.weight'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.2.out_layers.3.weight'], permute_conv=True)
    converted['diffusion']['unet.bottleneck.2.conv_merged.bias'] = to_tf_tensor(original_model['model.diffusion_model.middle_block.2.out_layers.3.bias'])

    # Decoder blocks (Residual and Attention)
    for i in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]: # Corresponding to output_blocks
        # Residual Block parts
        converted['diffusion']['unet.decoders.' + str(i) + '.0.groupnorm_feature.weight'] = to_tf_tensor(original_model['model.diffusion_model.output_blocks.' + str(i) + '.0.in_layers.0.weight'])
        converted['diffusion']['unet.decoders.' + str(i) + '.0.groupnorm_feature.bias'] = to_tf_tensor(original_model['model.diffusion_model.output_blocks.' + str(i) + '.0.in_layers.0.bias'])
        converted['diffusion']['unet.decoders.' + str(i) + '.0.conv_feature.weight'] = to_tf_tensor(original_model['model.diffusion_model.output_blocks.' + str(i) + '.0.in_layers.2.weight'], permute_conv=True)
        converted['diffusion']['unet.decoders.' + str(i) + '.0.conv_feature.bias'] = to_tf_tensor(original_model['model.diffusion_model.output_blocks.' + str(i) + '.0.in_layers.2.bias'])
        converted['diffusion']['unet.decoders.' + str(i) + '.0.linear_time.weight'] = to_tf_tensor(original_model['model.diffusion_model.output_blocks.' + str(i) + '.0.emb_layers.1.weight'], permute_linear=True)
        converted['diffusion']['unet.decoders.' + str(i) + '.0.linear_time.bias'] = to_tf_tensor(original_model['model.diffusion_model.output_blocks.' + str(i) + '.0.emb_layers.1.bias'])
        converted['diffusion']['unet.decoders.' + str(i) + '.0.groupnorm_merged.weight'] = to_tf_tensor(original_model['model.diffusion_model.output_blocks.' + str(i) + '.0.out_layers.0.weight'])
        converted['diffusion']['unet.decoders.' + str(i) + '.0.groupnorm_merged.bias'] = to_tf_tensor(original_model['model.diffusion_model.output_blocks.' + str(i) + '.0.out_layers.0.bias'])
        converted['diffusion']['unet.decoders.' + str(i) + '.0.conv_merged.weight'] = to_tf_tensor(original_model['model.diffusion_model.output_blocks.' + str(i) + '.0.out_layers.3.weight'], permute_conv=True)
        converted['diffusion']['unet.decoders.' + str(i) + '.0.conv_merged.bias'] = to_tf_tensor(original_model['model.diffusion_model.output_blocks.' + str(i) + '.0.out_layers.3.bias'])
        # Corrected the conditional check for skip_connection.weight
        skip_connection_weight_key = f'model.diffusion_model.output_blocks.{i}.0.skip_connection.weight'
        skip_connection_bias_key = f'model.diffusion_model.output_blocks.{i}.0.skip_connection.bias'
        if skip_connection_weight_key in original_model:
            converted['diffusion']['unet.decoders.' + str(i) + '.0.residual_layer.weight'] = to_tf_tensor(original_model[skip_connection_weight_key], permute_conv=True)
            converted['diffusion']['unet.decoders.' + str(i) + '.0.residual_layer.bias'] = to_tf_tensor(original_model[skip_connection_bias_key])

        # Attention Block parts (if applicable)
        if i not in [0, 1, 2]: # Blocks 3,4,5,6,7,8,9,10,11 have attention
            converted['diffusion']['unet.decoders.' + str(i) + '.1.groupnorm.weight'] = to_tf_tensor(original_model['model.diffusion_model.output_blocks.' + str(i) + '.1.norm.weight'])
            converted['diffusion']['unet.decoders.' + str(i) + '.1.groupnorm.bias'] = to_tf_tensor(original_model['model.diffusion_model.output_blocks.' + str(i) + '.1.norm.bias'])
            converted['diffusion']['unet.decoders.' + str(i) + '.1.conv_input.weight'] = to_tf_tensor(original_model['model.diffusion_model.output_blocks.' + str(i) + '.1.proj_in.weight'], permute_conv=True)
            converted['diffusion']['unet.decoders.' + str(i) + '.1.conv_input.bias'] = to_tf_tensor(original_model['model.diffusion_model.output_blocks.' + str(i) + '.1.proj_in.bias'])
            # In_proj weights for QKV are concatenated from 3 separate weights
            qkv_weight = torch.cat((original_model['model.diffusion_model.output_blocks.' + str(i) + '.1.transformer_blocks.0.attn1.to_q.weight'],
                                   original_model['model.diffusion_model.output_blocks.' + str(i) + '.1.transformer_blocks.0.attn1.to_k.weight'],
                                   original_model['model.diffusion_model.output_blocks.' + str(i) + '.1.transformer_blocks.0.attn1.to_v.weight']), 0)
            converted['diffusion']['unet.decoders.' + str(i) + '.1.attention_1.in_proj.weight'] = to_tf_tensor(qkv_weight, permute_linear=True)
            converted['diffusion']['unet.decoders.' + str(i) + '.1.attention_1.out_proj.weight'] = to_tf_tensor(original_model['model.diffusion_model.output_blocks.' + str(i) + '.1.transformer_blocks.0.attn1.to_out.0.weight'], permute_linear=True)
            converted['diffusion']['unet.decoders.' + str(i) + '.1.attention_1.out_proj.bias'] = to_tf_tensor(original_model['model.diffusion_model.output_blocks.' + str(i) + '.1.transformer_blocks.0.attn1.to_out.0.bias'])
            converted['diffusion']['unet.decoders.' + str(i) + '.1.linear_geglu_1.weight'] = to_tf_tensor(original_model['model.diffusion_model.output_blocks.' + str(i) + '.1.transformer_blocks.0.ff.net.0.proj.weight'], permute_linear=True)
            converted['diffusion']['unet.decoders.' + str(i) + '.1.linear_geglu_1.bias'] = to_tf_tensor(original_model['model.diffusion_model.output_blocks.' + str(i) + '.1.transformer_blocks.0.ff.net.0.proj.bias'])
            converted['diffusion']['unet.decoders.' + str(i) + '.1.linear_geglu_2.weight'] = to_tf_tensor(original_model['model.diffusion_model.output_blocks.' + str(i) + '.1.transformer_blocks.0.ff.net.2.weight'], permute_linear=True)
            converted['diffusion']['unet.decoders.' + str(i) + '.1.linear_geglu_2.bias'] = to_tf_tensor(original_model['model.diffusion_model.output_blocks.' + str(i) + '.1.transformer_blocks.0.ff.net.2.bias'])
            converted['diffusion']['unet.decoders.' + str(i) + '.1.attention_2.q_proj.weight'] = to_tf_tensor(original_model['model.diffusion_model.output_blocks.' + str(i) + '.1.transformer_blocks.0.attn2.to_q.weight'], permute_linear=True)
            converted['diffusion']['unet.decoders.' + str(i) + '.1.attention_2.k_proj.weight'] = to_tf_tensor(original_model['model.diffusion_model.output_blocks.' + str(i) + '.1.transformer_blocks.0.attn2.to_k.weight'], permute_linear=True)
            converted['diffusion']['unet.decoders.' + str(i) + '.1.attention_2.v_proj.weight'] = to_tf_tensor(original_model['model.diffusion_model.output_blocks.' + str(i) + '.1.transformer_blocks.0.attn2.to_v.weight'], permute_linear=True)
            converted['diffusion']['unet.decoders.' + str(i) + '.1.attention_2.out_proj.weight'] = to_tf_tensor(original_model['model.diffusion_model.output_blocks.' + str(i) + '.1.transformer_blocks.0.attn2.to_out.0.weight'], permute_linear=True)
            converted['diffusion']['unet.decoders.' + str(i) + '.1.attention_2.out_proj.bias'] = to_tf_tensor(original_model['model.diffusion_model.output_blocks.' + str(i) + '.1.transformer_blocks.0.attn2.to_out.0.bias'])
            converted['diffusion']['unet.decoders.' + str(i) + '.1.layernorm_1.weight'] = to_tf_tensor(original_model['model.diffusion_model.output_blocks.' + str(i) + '.1.transformer_blocks.0.norm1.weight'])
            converted['diffusion']['unet.decoders.' + str(i) + '.1.layernorm_1.bias'] = to_tf_tensor(original_model['model.diffusion_model.output_blocks.' + str(i) + '.1.transformer_blocks.0.norm1.bias'])
            converted['diffusion']['unet.decoders.' + str(i) + '.1.layernorm_2.weight'] = to_tf_tensor(original_model['model.diffusion_model.output_blocks.' + str(i) + '.1.transformer_blocks.0.norm2.weight'])
            converted['diffusion']['unet.decoders.' + str(i) + '.1.layernorm_2.bias'] = to_tf_tensor(original_model['model.diffusion_model.output_blocks.' + str(i) + '.1.transformer_blocks.0.norm2.bias'])
            converted['diffusion']['unet.decoders.' + str(i) + '.1.layernorm_3.weight'] = to_tf_tensor(original_model['model.diffusion_model.output_blocks.' + str(i) + '.1.transformer_blocks.0.norm3.weight'])
            converted['diffusion']['unet.decoders.' + str(i) + '.1.layernorm_3.bias'] = to_tf_tensor(original_model['model.diffusion_model.output_blocks.' + str(i) + '.1.transformer_blocks.0.norm3.bias'])
            converted['diffusion']['unet.decoders.' + str(i) + '.1.conv_output.weight'] = to_tf_tensor(original_model['model.diffusion_model.output_blocks.' + str(i) + '.1.proj_out.weight'], permute_conv=True)
            converted['diffusion']['unet.decoders.' + str(i) + '.1.conv_output.bias'] = to_tf_tensor(original_model['model.diffusion_model.output_blocks.' + str(i) + '.1.proj_out.bias'])

        # Upsample layers
        if i in [2, 5, 8]: # These blocks have an Upsample layer
            converted['diffusion']['unet.decoders.' + str(i) + '.2.conv.weight'] = to_tf_tensor(original_model['model.diffusion_model.output_blocks.' + str(i) + '.2.conv.weight'], permute_conv=True)
            converted['diffusion']['unet.decoders.' + str(i) + '.2.conv.bias'] = to_tf_tensor(original_model['model.diffusion_model.output_blocks.' + str(i) + '.2.conv.bias'])


    # Final layers
    converted['diffusion']['final.groupnorm.weight'] = to_tf_tensor(original_model['model.diffusion_model.out.0.weight'])
    converted['diffusion']['final.groupnorm.bias'] = to_tf_tensor(original_model['model.diffusion_model.out.0.bias'])
    converted['diffusion']['final.conv.weight'] = to_tf_tensor(original_model['model.diffusion_model.out.2.weight'], permute_conv=True)
    converted['diffusion']['final.conv.bias'] = to_tf_tensor(original_model['model.diffusion_model.out.2.bias'])

    # Encoder VAE weights
    converted['encoder']['0.weight'] = to_tf_tensor(original_model['first_stage_model.encoder.conv_in.weight'], permute_conv=True)
    converted['encoder']['0.bias'] = to_tf_tensor(original_model['first_stage_model.encoder.conv_in.bias'])

    # VAE Encoder Residual Blocks
    # Structure: down.0.block.0, down.0.block.1, down.1.block.0, down.1.block.1, down.2.block.0, down.2.block.1, down.3.block.0, down.3.block.1
    vae_encoder_blocks = [(0,0,1,2), (0,1,2,3), (1,0,4,5), (1,1,5,6), (2,0,7,8), (2,1,8,9), (3,0,10,11), (3,1,11,12)] # Pytorch down block index, Pytorch block index, TF block index, TF block index for next layer
    for pt_down_idx, pt_block_idx, tf_block_idx_1, tf_block_idx_2 in vae_encoder_blocks:
        converted['encoder'][str(tf_block_idx_1) + '.groupnorm_1.weight'] = to_tf_tensor(original_model[f'first_stage_model.encoder.down.{pt_down_idx}.block.{pt_block_idx}.norm1.weight'])
        converted['encoder'][str(tf_block_idx_1) + '.groupnorm_1.bias'] = to_tf_tensor(original_model[f'first_stage_model.encoder.down.{pt_down_idx}.block.{pt_block_idx}.norm1.bias'])
        converted['encoder'][str(tf_block_idx_1) + '.conv_1.weight'] = to_tf_tensor(original_model[f'first_stage_model.encoder.down.{pt_down_idx}.block.{pt_block_idx}.conv1.weight'], permute_conv=True)
        converted['encoder'][str(tf_block_idx_1) + '.conv_1.bias'] = to_tf_tensor(original_model[f'first_stage_model.encoder.down.{pt_down_idx}.block.{pt_block_idx}.conv1.bias'])
        converted['encoder'][str(tf_block_idx_1) + '.groupnorm_2.weight'] = to_tf_tensor(original_model[f'first_stage_model.encoder.down.{pt_down_idx}.block.{pt_block_idx}.norm2.weight'])
        converted['encoder'][str(tf_block_idx_1) + '.groupnorm_2.bias'] = to_tf_tensor(original_model[f'first_stage_model.encoder.down.{pt_down_idx}.block.{pt_block_idx}.norm2.bias'])
        converted['encoder'][str(tf_block_idx_1) + '.conv_2.weight'] = to_tf_tensor(original_model[f'first_stage_model.encoder.down.{pt_down_idx}.block.{pt_block_idx}.conv2.weight'], permute_conv=True)
        converted['encoder'][str(tf_block_idx_1) + '.conv_2.bias'] = to_tf_tensor(original_model[f'first_stage_model.encoder.down.{pt_down_idx}.block.{pt_block_idx}.conv2.bias'])
        if pt_down_idx in [1, 2]: # Only for specific blocks, these have shortcut conv
            if f'first_stage_model.encoder.down.{pt_down_idx}.block.{pt_block_idx}.nin_shortcut.weight' in original_model:
                converted['encoder'][str(tf_block_idx_1) + '.residual_layer.weight'] = to_tf_tensor(original_model[f'first_stage_model.encoder.down.{pt_down_idx}.block.{pt_block_idx}.nin_shortcut.weight'], permute_conv=True)
                converted['encoder'][str(tf_block_idx_1) + '.residual_layer.bias'] = to_tf_tensor(original_model[f'first_stage_model.encoder.down.{pt_down_idx}.block.{pt_block_idx}.nin_shortcut.bias'])

    # VAE Encoder Downsample layers (3, 6, 9 in TF)
    converted['encoder']['3.weight'] = to_tf_tensor(original_model['first_stage_model.encoder.down.0.downsample.conv.weight'], permute_conv=True)
    converted['encoder']['3.bias'] = to_tf_tensor(original_model['first_stage_model.encoder.down.0.downsample.conv.bias'])
    converted['encoder']['6.weight'] = to_tf_tensor(original_model['first_stage_model.encoder.down.1.downsample.conv.weight'], permute_conv=True)
    converted['encoder']['6.bias'] = to_tf_tensor(original_model['first_stage_model.encoder.down.1.downsample.conv.bias'])
    converted['encoder']['9.weight'] = to_tf_tensor(original_model['first_stage_model.encoder.down.2.downsample.conv.weight'], permute_conv=True)
    converted['encoder']['9.bias'] = to_tf_tensor(original_model['first_stage_model.encoder.down.2.downsample.conv.bias'])

    # VAE Encoder Middle Block
    converted['encoder']['12.groupnorm_1.weight'] = to_tf_tensor(original_model['first_stage_model.encoder.mid.block_1.norm1.weight'])
    converted['encoder']['12.groupnorm_1.bias'] = to_tf_tensor(original_model['first_stage_model.encoder.mid.block_1.norm1.bias'])
    converted['encoder']['12.conv_1.weight'] = to_tf_tensor(original_model['first_stage_model.encoder.mid.block_1.conv1.weight'], permute_conv=True)
    converted['encoder']['12.conv_1.bias'] = to_tf_tensor(original_model['first_stage_model.encoder.mid.block_1.conv1.bias'])
    converted['encoder']['12.groupnorm_2.weight'] = to_tf_tensor(original_model['first_stage_model.encoder.mid.block_1.norm2.weight'])
    converted['encoder']['12.groupnorm_2.bias'] = to_tf_tensor(original_model['first_stage_model.encoder.mid.block_1.norm2.bias'])
    converted['encoder']['12.conv_2.weight'] = to_tf_tensor(original_model['first_stage_model.encoder.mid.block_1.conv2.weight'], permute_conv=True)
    converted['encoder']['12.conv_2.bias'] = to_tf_tensor(original_model['first_stage_model.encoder.mid.block_1.conv2.bias'])

    converted['encoder']['13.groupnorm.weight'] = to_tf_tensor(original_model['first_stage_model.encoder.mid.attn_1.norm.weight'])
    converted['encoder']['13.groupnorm.bias'] = to_tf_tensor(original_model['first_stage_model.encoder.mid.attn_1.norm.bias'])
    qkv_weight = torch.cat((original_model['first_stage_model.encoder.mid.attn_1.q.weight'], original_model['first_stage_model.encoder.mid.attn_1.k.weight'], original_model['first_stage_model.encoder.mid.attn_1.v.weight']), 0)
    converted['encoder']['13.attention.in_proj.weight'] = to_tf_tensor(qkv_weight, permute_linear=True)
    qkv_bias = torch.cat((original_model['first_stage_model.encoder.mid.attn_1.q.bias'], original_model['first_stage_model.encoder.mid.attn_1.k.bias'], original_model['first_stage_model.encoder.mid.attn_1.v.bias']), 0)
    converted['encoder']['13.attention.in_proj.bias'] = to_tf_tensor(qkv_bias)
    converted['encoder']['13.attention.out_proj.weight'] = to_tf_tensor(original_model['first_stage_model.encoder.mid.attn_1.proj_out.weight'], permute_linear=True)
    converted['encoder']['13.attention.out_proj.bias'] = to_tf_tensor(original_model['first_stage_model.encoder.mid.attn_1.proj_out.bias'])

    converted['encoder']['14.groupnorm_1.weight'] = to_tf_tensor(original_model['first_stage_model.encoder.mid.block_2.norm1.weight'])
    converted['encoder']['14.groupnorm_1.bias'] = to_tf_tensor(original_model['first_stage_model.encoder.mid.block_2.norm1.bias'])
    converted['encoder']['14.conv_1.weight'] = to_tf_tensor(original_model['first_stage_model.encoder.mid.block_2.conv1.weight'], permute_conv=True)
    converted['encoder']['14.conv_1.bias'] = to_tf_tensor(original_model['first_stage_model.encoder.mid.block_2.conv1.bias'])
    converted['encoder']['14.groupnorm_2.weight'] = to_tf_tensor(original_model['first_stage_model.encoder.mid.block_2.norm2.weight'])
    converted['encoder']['14.groupnorm_2.bias'] = to_tf_tensor(original_model['first_stage_model.encoder.mid.block_2.norm2.bias'])
    converted['encoder']['14.conv_2.weight'] = to_tf_tensor(original_model['first_stage_model.encoder.mid.block_2.conv2.weight'], permute_conv=True)
    converted['encoder']['14.conv_2.bias'] = to_tf_tensor(original_model['first_stage_model.encoder.mid.block_2.conv2.bias'])

    converted['encoder']['15.weight'] = to_tf_tensor(original_model['first_stage_model.encoder.norm_out.weight'])
    converted['encoder']['15.bias'] = to_tf_tensor(original_model['first_stage_model.encoder.norm_out.bias'])
    converted['encoder']['17.weight'] = to_tf_tensor(original_model['first_stage_model.encoder.conv_out.weight'], permute_conv=True)
    converted['encoder']['17.bias'] = to_tf_tensor(original_model['first_stage_model.encoder.conv_out.bias'])
    converted['encoder']['18.weight'] = to_tf_tensor(original_model['first_stage_model.quant_conv.weight'], permute_conv=True)
    converted['encoder']['18.bias'] = to_tf_tensor(original_model['first_stage_model.quant_conv.bias'])

    # VAE Decoder weights
    converted['decoder']['0.weight'] = to_tf_tensor(original_model['first_stage_model.post_quant_conv.weight'], permute_conv=True)
    converted['decoder']['0.bias'] = to_tf_tensor(original_model['first_stage_model.post_quant_conv.bias'])
    converted['decoder']['1.weight'] = to_tf_tensor(original_model['first_stage_model.decoder.conv_in.weight'], permute_conv=True)
    converted['decoder']['1.bias'] = to_tf_tensor(original_model['first_stage_model.decoder.conv_in.bias'])

    # VAE Decoder Middle Block
    converted['decoder']['2.groupnorm_1.weight'] = to_tf_tensor(original_model['first_stage_model.decoder.mid.block_1.norm1.weight'])
    converted['decoder']['2.groupnorm_1.bias'] = to_tf_tensor(original_model['first_stage_model.decoder.mid.block_1.norm1.bias'])
    converted['decoder']['2.conv_1.weight'] = to_tf_tensor(original_model['first_stage_model.decoder.mid.block_1.conv1.weight'], permute_conv=True)
    converted['decoder']['2.conv_1.bias'] = to_tf_tensor(original_model['first_stage_model.decoder.mid.block_1.conv1.bias'])
    converted['decoder']['2.groupnorm_2.weight'] = to_tf_tensor(original_model['first_stage_model.decoder.mid.block_1.norm2.weight'])
    converted['decoder']['2.groupnorm_2.bias'] = to_tf_tensor(original_model['first_stage_model.decoder.mid.block_1.norm2.bias'])
    converted['decoder']['2.conv_2.weight'] = to_tf_tensor(original_model['first_stage_model.decoder.mid.block_1.conv2.weight'], permute_conv=True)
    converted['decoder']['2.conv_2.bias'] = to_tf_tensor(original_model['first_stage_model.decoder.mid.block_1.conv2.bias'])

    converted['decoder']['3.groupnorm.weight'] = to_tf_tensor(original_model['first_stage_model.decoder.mid.attn_1.norm.weight'])
    converted['decoder']['3.groupnorm.bias'] = to_tf_tensor(original_model['first_stage_model.decoder.mid.attn_1.norm.bias'])
    qkv_weight = torch.cat((original_model['first_stage_model.decoder.mid.attn_1.q.weight'], original_model['first_stage_model.decoder.mid.attn_1.k.weight'], original_model['first_stage_model.decoder.mid.attn_1.v.weight']), 0)
    converted['decoder']['3.attention.in_proj.weight'] = to_tf_tensor(qkv_weight, permute_linear=True)
    qkv_bias = torch.cat((original_model['first_stage_model.decoder.mid.attn_1.q.bias'], original_model['first_stage_model.decoder.mid.attn_1.k.bias'], original_model['first_stage_model.decoder.mid.attn_1.v.bias']), 0)
    converted['decoder']['3.attention.in_proj.bias'] = to_tf_tensor(qkv_bias)
    converted['decoder']['3.attention.out_proj.weight'] = to_tf_tensor(original_model['first_stage_model.decoder.mid.attn_1.proj_out.weight'], permute_linear=True)
    converted['decoder']['3.attention.out_proj.bias'] = to_tf_tensor(original_model['first_stage_model.decoder.mid.attn_1.proj_out.bias'])

    converted['decoder']['4.groupnorm_1.weight'] = to_tf_tensor(original_model['first_stage_model.decoder.mid.block_2.norm1.weight'])
    converted['decoder']['4.groupnorm_1.bias'] = to_tf_tensor(original_model['first_stage_model.decoder.mid.block_2.norm1.bias'])
    converted['decoder']['4.conv_1.weight'] = to_tf_tensor(original_model['first_stage_model.decoder.mid.block_2.conv1.weight'], permute_conv=True)
    converted['decoder']['4.conv_1.bias'] = to_tf_tensor(original_model['first_stage_model.decoder.mid.block_2.conv1.bias'])
    converted['decoder']['4.groupnorm_2.weight'] = to_tf_tensor(original_model['first_stage_model.decoder.mid.block_2.norm2.weight'])
    converted['decoder']['4.groupnorm_2.bias'] = to_tf_tensor(original_model['first_stage_model.decoder.mid.block_2.norm2.bias'])
    converted['decoder']['4.conv_2.weight'] = to_tf_tensor(original_model['first_stage_model.decoder.mid.block_2.conv2.weight'], permute_conv=True)
    converted['decoder']['4.conv_2.bias'] = to_tf_tensor(original_model['first_stage_model.decoder.mid.block_2.conv2.bias'])

    # VAE Decoder Up Blocks
    # up.0.block.0, up.0.block.1, up.0.block.2 -> TF layers 20, 21, 22
    # up.1.block.0, up.1.block.1, up.1.block.2 -> TF layers 15, 16, 17
    # up.2.block.0, up.2.block.1, up.2.block.2 -> TF layers 10, 11, 12
    # up.3.block.0, up.3.block.1, up.3.block.2 -> TF layers 5, 6, 7
    vae_decoder_up_blocks = [
        (0, 0, 20), (0, 1, 21), (0, 2, 22),
        (1, 0, 15), (1, 1, 16), (1, 2, 17),
        (2, 0, 10), (2, 1, 11), (2, 2, 12),
        (3, 0, 5), (3, 1, 6), (3, 2, 7),
    ]
    for pt_up_idx, pt_block_idx, tf_block_idx in vae_decoder_up_blocks:
        converted['decoder'][str(tf_block_idx) + '.groupnorm_1.weight'] = to_tf_tensor(original_model[f'first_stage_model.decoder.up.{pt_up_idx}.block.{pt_block_idx}.norm1.weight'])
        converted['decoder'][str(tf_block_idx) + '.groupnorm_1.bias'] = to_tf_tensor(original_model[f'first_stage_model.decoder.up.{pt_up_idx}.block.{pt_block_idx}.norm1.bias'])
        converted['decoder'][str(tf_block_idx) + '.conv_1.weight'] = to_tf_tensor(original_model[f'first_stage_model.decoder.up.{pt_up_idx}.block.{pt_block_idx}.conv1.weight'], permute_conv=True)
        converted['decoder'][str(tf_block_idx) + '.conv_1.bias'] = to_tf_tensor(original_model[f'first_stage_model.decoder.up.{pt_up_idx}.block.{pt_block_idx}.conv1.bias'])
        converted['decoder'][str(tf_block_idx) + '.groupnorm_2.weight'] = to_tf_tensor(original_model[f'first_stage_model.decoder.up.{pt_up_idx}.block.{pt_block_idx}.norm2.weight'])
        converted['decoder'][str(tf_block_idx) + '.groupnorm_2.bias'] = to_tf_tensor(original_model[f'first_stage_model.decoder.up.{pt_up_idx}.block.{pt_block_idx}.norm2.bias'])
        converted['decoder'][str(tf_block_idx) + '.conv_2.weight'] = to_tf_tensor(original_model[f'first_stage_model.decoder.up.{pt_up_idx}.block.{pt_block_idx}.conv2.weight'], permute_conv=True)
        converted['decoder'][str(tf_block_idx) + '.conv_2.bias'] = to_tf_tensor(original_model[f'first_stage_model.decoder.up.{pt_up_idx}.block.{pt_block_idx}.conv2.bias'])
        if pt_up_idx in [0,1]: # The up.0 and up.1 blocks have residual shortcuts
            if f'first_stage_model.decoder.up.{pt_up_idx}.block.{pt_block_idx}.nin_shortcut.weight' in original_model:
                converted['decoder'][str(tf_block_idx) + '.residual_layer.weight'] = to_tf_tensor(original_model[f'first_stage_model.decoder.up.{pt_up_idx}.block.{pt_block_idx}.nin_shortcut.weight'], permute_conv=True)
                converted['decoder'][str(tf_block_idx) + '.residual_layer.bias'] = to_tf_tensor(original_model[f'first_stage_model.decoder.up.{pt_up_idx}.block.{pt_block_idx}.nin_shortcut.bias'])

    # VAE Decoder Upsample layers
    converted['decoder']['19.weight'] = to_tf_tensor(original_model['first_stage_model.decoder.up.1.upsample.conv.weight'], permute_conv=True)
    converted['decoder']['19.bias'] = to_tf_tensor(original_model['first_stage_model.decoder.up.1.upsample.conv.bias'])
    converted['decoder']['14.weight'] = to_tf_tensor(original_model['first_stage_model.decoder.up.2.upsample.conv.weight'], permute_conv=True)
    converted['decoder']['14.bias'] = to_tf_tensor(original_model['first_stage_model.decoder.up.2.upsample.conv.bias'])
    converted['decoder']['9.weight'] = to_tf_tensor(original_model['first_stage_model.decoder.up.3.upsample.conv.weight'], permute_conv=True)
    converted['decoder']['9.bias'] = to_tf_tensor(original_model['first_stage_model.decoder.up.3.upsample.conv.bias'])

    converted['decoder']['23.weight'] = to_tf_tensor(original_model['first_stage_model.decoder.norm_out.weight'])
    converted['decoder']['23.bias'] = to_tf_tensor(original_model['first_stage_model.decoder.norm_out.bias'])
    converted['decoder']['25.weight'] = to_tf_tensor(original_model['first_stage_model.decoder.conv_out.weight'], permute_conv=True)
    converted['decoder']['25.bias'] = to_tf_tensor(original_model['first_stage_model.decoder.conv_out.bias'])

    # CLIP Model Weights
    converted['clip']['embedding.token_embedding.weight'] = to_tf_tensor(original_model['cond_stage_model.transformer.text_model.embeddings.token_embedding.weight'])
    converted['clip']['embedding.position_embedding'] = to_tf_tensor(original_model['cond_stage_model.transformer.text_model.embeddings.position_embedding.weight'])

    for i in range(12): # 12 CLIP layers
        # Self-attention In_proj (QKV concatenated)
        qkv_weight = torch.cat((
            original_model[f'cond_stage_model.transformer.text_model.encoder.layers.{i}.self_attn.q_proj.weight'],
            original_model[f'cond_stage_model.transformer.text_model.encoder.layers.{i}.self_attn.k_proj.weight'],
            original_model[f'cond_stage_model.transformer.text_model.encoder.layers.{i}.self_attn.v_proj.weight']
        ), 0)
        converted['clip']['layers.' + str(i) + '.attention.in_proj.weight'] = to_tf_tensor(qkv_weight, permute_linear=True)
        qkv_bias = torch.cat((
            original_model[f'cond_stage_model.transformer.text_model.encoder.layers.{i}.self_attn.q_proj.bias'],
            original_model[f'cond_stage_model.transformer.text_model.encoder.layers.{i}.self_attn.k_proj.bias'],
            original_model[f'cond_stage_model.transformer.text_model.encoder.layers.{i}.self_attn.v_proj.bias']
        ), 0)
        converted['clip']['layers.' + str(i) + '.attention.in_proj.bias'] = to_tf_tensor(qkv_bias)

        converted['clip']['layers.' + str(i) + '.attention.out_proj.weight'] = to_tf_tensor(original_model[f'cond_stage_model.transformer.text_model.encoder.layers.{i}.self_attn.out_proj.weight'], permute_linear=True)
        converted['clip']['layers.' + str(i) + '.attention.out_proj.bias'] = to_tf_tensor(original_model[f'cond_stage_model.transformer.text_model.encoder.layers.{i}.self_attn.out_proj.bias'])
        converted['clip']['layers.' + str(i) + '.layernorm_1.weight'] = to_tf_tensor(original_model[f'cond_stage_model.transformer.text_model.encoder.layers.{i}.layer_norm1.weight'])
        converted['clip']['layers.' + str(i) + '.layernorm_1.bias'] = to_tf_tensor(original_model[f'cond_stage_model.transformer.text_model.encoder.layers.{i}.layer_norm1.bias'])
        converted['clip']['layers.' + str(i) + '.linear_1.weight'] = to_tf_tensor(original_model[f'cond_stage_model.transformer.text_model.encoder.layers.{i}.mlp.fc1.weight'], permute_linear=True)
        converted['clip']['layers.' + str(i) + '.linear_1.bias'] = to_tf_tensor(original_model[f'cond_stage_model.transformer.text_model.encoder.layers.{i}.mlp.fc1.bias'])
        converted['clip']['layers.' + str(i) + '.linear_2.weight'] = to_tf_tensor(original_model[f'cond_stage_model.transformer.text_model.encoder.layers.{i}.mlp.fc2.weight'], permute_linear=True)
        converted['clip']['layers.' + str(i) + '.linear_2.bias'] = to_tf_tensor(original_model[f'cond_stage_model.transformer.text_model.encoder.layers.{i}.mlp.fc2.bias'])
        converted['clip']['layers.' + str(i) + '.layernorm_2.weight'] = to_tf_tensor(original_model[f'cond_stage_model.transformer.text_model.encoder.layers.{i}.layer_norm2.weight'])
        converted['clip']['layers.' + str(i) + '.layernorm_2.bias'] = to_tf_tensor(original_model[f'cond_stage_model.transformer.text_model.encoder.layers.{i}.layer_norm2.bias'])

    converted['clip']['layernorm.weight'] = to_tf_tensor(original_model['cond_stage_model.transformer.text_model.final_layer_norm.weight'])
    converted['clip']['layernorm.bias'] = to_tf_tensor(original_model['cond_stage_model.transformer.text_model.final_layer_norm.bias'])

    return converted

import tensorflow as tf

def preload_models_from_standard_weights(ckpt_path, device):
    state_dict = load_from_standard_weights(ckpt_path, device)

    # Instantiate models
    encoder = VAE_Encoder()
    decoder = VAE_Decoder()
    diffusion = Diffusion()
    clip = CLIP()

    # Build models with dummy inputs to create weights
    # Encoder requires (image, noise)
    dummy_image = tf.random.normal([1, 3, 512, 512], dtype=tf.float32) # NCHW
    dummy_noise_shape = (1, 4, 64, 64) # Batch, Channels, H, W for VAE latent space
    dummy_noise = tf.random.normal(dummy_noise_shape, dtype=tf.float32)
    encoder(dummy_image, dummy_noise)

    # Decoder requires (latent)
    dummy_latent = tf.random.normal(dummy_noise_shape, dtype=tf.float32)
    decoder(dummy_latent)

    # Diffusion requires (latent, context, time)
    dummy_context = tf.random.normal([1, 77, 768], dtype=tf.float32)
    dummy_time = tf.constant([0], dtype=tf.int32) # Time embedding needs to be generated by get_time_embedding
    dummy_time_embedding = get_time_embedding(dummy_time)
    diffusion(dummy_latent, dummy_context, dummy_time_embedding)

    # Clip requires (tokens)
    dummy_tokens = tf.constant([[0] * 77], dtype=tf.int32)
    clip(dummy_tokens)

    # Helper to set weights for layers with 'weight' and 'bias'
    def set_weights_for_layer(layer, weights_dict_prefix):
        weights_to_set = []
        if weights_dict_prefix + '.weight' in state_dict[model_key]:
            weights_to_set.append(state_dict[model_key][weights_dict_prefix + '.weight'])
        if weights_dict_prefix + '.bias' in state_dict[model_key]:
            weights_to_set.append(state_dict[model_key][weights_dict_prefix + '.bias'])
        if weights_to_set:
            layer.set_weights(weights_to_set)

    # Assign weights to VAE Encoder
    model_key = 'encoder'
    encoder.layers_list[0].set_weights([
        state_dict[model_key]['0.weight'],
        state_dict[model_key]['0.bias']
    ])
    # VAE_ResidualBlock weights
    vae_encoder_tf_blocks = [1, 2, 4, 5, 7, 8, 10, 11, 12, 14] # Indices in layers_list
    for i, tf_idx in enumerate(vae_encoder_tf_blocks):
        block = encoder.layers_list[tf_idx]
        block_prefix = str(tf_idx)

        block.groupNorm1.set_weights([
            state_dict[model_key][block_prefix + '.groupnorm_1.weight'],
            state_dict[model_key][block_prefix + '.groupnorm_1.bias']
        ])
        block.conv1.set_weights([
            state_dict[model_key][block_prefix + '.conv_1.weight'],
            state_dict[model_key][block_prefix + '.conv_1.bias']
        ])
        block.groupNorm2.set_weights([
            state_dict[model_key][block_prefix + '.groupnorm_2.weight'],
            state_dict[model_key][block_prefix + '.groupnorm_2.bias']
        ])
        block.conv2.set_weights([
            state_dict[model_key][block_prefix + '.conv_2.weight'],
            state_dict[model_key][block_prefix + '.conv_2.bias']
        ])
        if isinstance(block.resual_layer, tf.keras.layers.Conv2D):
            block.resual_layer.set_weights([
                state_dict[model_key][block_prefix + '.residual_layer.weight'],
                state_dict[model_key][block_prefix + '.residual_layer.bias']
            ])

    # VAE Encoder Downsample layers
    for tf_idx in [3, 6, 9]:
        encoder.layers_list[tf_idx].set_weights([
            state_dict[model_key][str(tf_idx) + '.weight'],
            state_dict[model_key][str(tf_idx) + '.bias']
        ])

    # VAE Encoder Attention Block (index 13)
    attn_block = encoder.layers_list[13]
    attn_block.groupNorm.set_weights([
        state_dict[model_key]['13.groupnorm.weight'],
        state_dict[model_key]['13.groupnorm.bias']
    ])
    attn_block.attention.in_proj_bias.set_weights([
        state_dict[model_key]['13.attention.in_proj.weight'],
        state_dict[model_key]['13.attention.in_proj.bias']
    ])
    attn_block.attention.out_proj.set_weights([
        state_dict[model_key]['13.attention.out_proj.weight'],
        state_dict[model_key]['13.attention.out_proj.bias']
    ])

    # Final layers of VAE Encoder
    encoder.layers_list[15].set_weights([
        state_dict[model_key]['15.weight'],
        state_dict[model_key]['15.bias']
    ])
    encoder.layers_list[17].set_weights([
        state_dict[model_key]['17.weight'],
        state_dict[model_key]['17.bias']
    ])
    encoder.layers_list[18].set_weights([
        state_dict[model_key]['18.weight'],
        state_dict[model_key]['18.bias']
    ])

    # Assign weights to VAE Decoder
    model_key = 'decoder'
    decoder.layers_list[0].set_weights([
        state_dict[model_key]['0.weight'],
        state_dict[model_key]['0.bias']
    ])
    decoder.layers_list[1].set_weights([
        state_dict[model_key]['1.weight'],
        state_dict[model_key]['1.bias']
    ])

    # VAE Decoder middle block (indices 2, 3, 4)
    # Block 2 (Residual)
    dec_mid_block_2 = decoder.layers_list[2]
    dec_mid_block_2.groupNorm1.set_weights([
        state_dict[model_key]['2.groupnorm_1.weight'],
        state_dict[model_key]['2.groupnorm_1.bias']
    ])
    dec_mid_block_2.conv1.set_weights([
        state_dict[model_key]['2.conv_1.weight'],
        state_dict[model_key]['2.conv_1.bias']
    ])
    dec_mid_block_2.groupNorm2.set_weights([
        state_dict[model_key]['2.groupnorm_2.weight'],
        state_dict[model_key]['2.groupnorm_2.bias']
    ])
    dec_mid_block_2.conv2.set_weights([
        state_dict[model_key]['2.conv_2.weight'],
        state_dict[model_key]['2.conv_2.bias']
    ])
    # Block 3 (Attention)
    dec_mid_block_3 = decoder.layers_list[3]
    dec_mid_block_3.groupNorm.set_weights([
        state_dict[model_key]['3.groupnorm.weight'],
        state_dict[model_key]['3.groupnorm.bias']
    ])
    dec_mid_block_3.attention.in_proj_bias.set_weights([
        state_dict[model_key]['3.attention.in_proj.weight'],
        state_dict[model_key]['3.attention.in_proj.bias']
    ])
    dec_mid_block_3.attention.out_proj.set_weights([
        state_dict[model_key]['3.attention.out_proj.weight'],
        state_dict[model_key]['3.attention.out_proj.bias']
    ])
    # Block 4 (Residual)
    dec_mid_block_4 = decoder.layers_list[4]
    dec_mid_block_4.groupNorm1.set_weights([
        state_dict[model_key]['4.groupnorm_1.weight'],
        state_dict[model_key]['4.groupnorm_1.bias']
    ])
    dec_mid_block_4.conv1.set_weights([
        state_dict[model_key]['4.conv_1.weight'],
        state_dict[model_key]['4.conv_1.bias']
    ])
    dec_mid_block_4.groupNorm2.set_weights([
        state_dict[model_key]['4.groupnorm_2.weight'],
        state_dict[model_key]['4.groupnorm_2.bias']
    ])
    dec_mid_block_4.conv2.set_weights([
        state_dict[model_key]['4.conv_2.weight'],
        state_dict[model_key]['4.conv_2.bias']
    ])

    # VAE Decoder Up Blocks (Residual blocks and Upsampling)
    vae_decoder_tf_blocks = [
        (5, 6, 7), # Corresponds to up.3.block.0, up.3.block.1, up.3.block.2
        (10, 11, 12), # Corresponds to up.2.block.0, up.2.block.1, up.2.block.2
        (15, 16, 17), # Corresponds to up.1.block.0, up.1.block.1, up.1.block.2
        (20, 21, 22)  # Corresponds to up.0.block.0, up.0.block.1, up.0.block.2
    ]

    for block_indices in vae_decoder_tf_blocks:
        for tf_idx in block_indices:
            block = decoder.layers_list[tf_idx]
            block_prefix = str(tf_idx)

            block.groupNorm1.set_weights([
                state_dict[model_key][block_prefix + '.groupnorm_1.weight'],
                state_dict[model_key][block_prefix + '.groupnorm_1.bias']
            ])
            block.conv1.set_weights([
                state_dict[model_key][block_prefix + '.conv_1.weight'],
                state_dict[model_key][block_prefix + '.conv_1.bias']
            ])
            block.groupNorm2.set_weights([
                state_dict[model_key][block_prefix + '.groupnorm_2.weight'],
                state_dict[model_key][block_prefix + '.groupnorm_2.bias']
            ])
            block.conv2.set_weights([
                state_dict[model_key][block_prefix + '.conv_2.weight'],
                state_dict[model_key][block_prefix + '.conv_2.bias']
            ])
            if isinstance(block.resual_layer, tf.keras.layers.Conv2D):
                block.resual_layer.set_weights([
                    state_dict[model_key][block_prefix + '.residual_layer.weight'],
                    state_dict[model_key][block_prefix + '.residual_layer.bias']
                ])

    # VAE Decoder Upsampling layers
    for tf_idx in [9, 14, 19]: # Indices of Conv2D after UpSampling2D
        upsample_layer = decoder.layers_list[tf_idx]
        upsample_layer.set_weights([
            state_dict[model_key][str(tf_idx) + '.weight'],
            state_dict[model_key][str(tf_idx) + '.bias']
        ])

    # Final layers of VAE Decoder
    decoder.layers_list[23].set_weights([
        state_dict[model_key]['23.weight'],
        state_dict[model_key]['23.bias']
    ])
    decoder.layers_list[25].set_weights([
        state_dict[model_key]['25.weight'],
        state_dict[model_key]['25.bias']
    ])

    # Assign weights to Diffusion UNET
    model_key = 'diffusion'
    diffusion.time_embedding.linear1.set_weights([
        state_dict[model_key]['time_embedding.linear_1.weight'],
        state_dict[model_key]['time_embedding.linear_1.bias']
    ])
    diffusion.time_embedding.linear2.set_weights([
        state_dict[model_key]['time_embedding.linear_2.weight'],
        state_dict[model_key]['time_embedding.linear_2.bias']
    ])

    # UNET Encoders
    unet_encoders_tf_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11] # Indices in diffusion.unet.encoders
    for enc_idx in unet_encoders_tf_indices:
        encoder_block = diffusion.unet.encoders[enc_idx].layers_list[0]
        prefix = 'unet.encoders.' + str(enc_idx)

        if isinstance(encoder_block, tf.keras.layers.Conv2D):
            encoder_block.set_weights([
                state_dict[model_key][prefix + '.0.weight'],
                state_dict[model_key][prefix + '.0.bias']
            ])
        elif isinstance(encoder_block, UNET_ResidualBlock):
            encoder_block.groupnorm.set_weights([
                state_dict[model_key][prefix + '.0.groupnorm_feature.weight'],
                state_dict[model_key][prefix + '.0.groupnorm_feature.bias']
            ])
            encoder_block.conv_feature.set_weights([
                state_dict[model_key][prefix + '.0.conv_feature.weight'],
                state_dict[model_key][prefix + '.0.conv_feature.bias']
            ])
            encoder_block.linear_time.set_weights([
                state_dict[model_key][prefix + '.0.linear_time.weight'],
                state_dict[model_key][prefix + '.0.linear_time.bias']
            ])
            encoder_block.groupnorm_merged.set_weights([
                state_dict[model_key][prefix + '.0.groupnorm_merged.weight'],
                state_dict[model_key][prefix + '.0.groupnorm_merged.bias']
            ])
            encoder_block.conv_merged.set_weights([
                state_dict[model_key][prefix + '.0.conv_merged.weight'],
                state_dict[model_key][prefix + '.0.conv_merged.bias']
            ])
            if isinstance(encoder_block.residual_layer, tf.keras.layers.Conv2D):
                encoder_block.residual_layer.set_weights([
                    state_dict[model_key][prefix + '.0.residual_layer.weight'],
                    state_dict[model_key][prefix + '.0.residual_layer.bias']
                ])

        if len(diffusion.unet.encoders[enc_idx].layers_list) > 1 and isinstance(diffusion.unet.encoders[enc_idx].layers_list[1], UNET_AttentionBlock):
            attn_block = diffusion.unet.encoders[enc_idx].layers_list[1]
            attn_prefix = prefix + '.1'
            attn_block.groupnorm.set_weights([
                state_dict[model_key][attn_prefix + '.groupnorm.weight'],
                state_dict[model_key][attn_prefix + '.groupnorm.bias']
            ])
            attn_block.conv_input.set_weights([
                state_dict[model_key][attn_prefix + '.conv_input.weight'],
                state_dict[model_key][attn_prefix + '.conv_input.bias']
            ])
            attn_block.layernorm_1.set_weights([
                state_dict[model_key][attn_prefix + '.layernorm_1.weight'],
                state_dict[model_key][attn_prefix + '.layernorm_1.bias']
            ])
            attn_block.attention1.in_proj_bias.set_weights([
                state_dict[model_key][attn_prefix + '.attention_1.in_proj.weight']
            ])
            attn_block.attention1.out_proj.set_weights([
                state_dict[model_key][attn_prefix + '.attention_1.out_proj.weight'],
                state_dict[model_key][attn_prefix + '.attention_1.out_proj.bias']
            ])
            attn_block.layernorm_2.set_weights([
                state_dict[model_key][attn_prefix + '.layernorm_2.weight'],
                state_dict[model_key][attn_prefix + '.layernorm_2.bias']
            ])
            attn_block.attention2.q_proj.set_weights([
                state_dict[model_key][attn_prefix + '.attention_2.q_proj.weight']
            ])
            attn_block.attention2.k_proj.set_weights([
                state_dict[model_key][attn_prefix + '.attention_2.k_proj.weight']
            ])
            attn_block.attention2.v_proj.set_weights([
                state_dict[model_key][attn_prefix + '.attention_2.v_proj.weight']
            ])
            attn_block.attention2.out_proj.set_weights([
                state_dict[model_key][attn_prefix + '.attention_2.out_proj.weight'],
                state_dict[model_key][attn_prefix + '.attention_2.out_proj.bias']
            ])
            attn_block.layernorm_3.set_weights([
                state_dict[model_key][attn_prefix + '.layernorm_3.weight'],
                state_dict[model_key][attn_prefix + '.layernorm_3.bias']
            ])
            attn_block.linear_geglu_1.set_weights([
                state_dict[model_key][attn_prefix + '.linear_geglu_1.weight'],
                state_dict[model_key][attn_prefix + '.linear_geglu_1.bias']
            ])
            attn_block.linear_geglu_2.set_weights([
                state_dict[model_key][attn_prefix + '.linear_geglu_2.weight'],
                state_dict[model_key][attn_prefix + '.linear_geglu_2.bias']
            ])
            attn_block.conv_output.set_weights([
                state_dict[model_key][attn_prefix + '.conv_output.weight'],
                state_dict[model_key][attn_prefix + '.conv_output.bias']
            ])

    # UNET Bottleneck
    bottleneck_layers = diffusion.unet.bottleneck.layers_list
    # Residual block 0
    res_block_0 = bottleneck_layers[0]
    res_block_0.groupnorm.set_weights([
        state_dict[model_key]['unet.bottleneck.0.groupnorm_feature.weight'],
        state_dict[model_key]['unet.bottleneck.0.groupnorm_feature.bias']
    ])
    res_block_0.conv_feature.set_weights([
        state_dict[model_key]['unet.bottleneck.0.conv_feature.weight'],
        state_dict[model_key]['unet.bottleneck.0.conv_feature.bias']
    ])
    res_block_0.linear_time.set_weights([
        state_dict[model_key]['unet.bottleneck.0.linear_time.weight'],
        state_dict[model_key]['unet.bottleneck.0.linear_time.bias']
    ])
    res_block_0.groupnorm_merged.set_weights([
        state_dict[model_key]['unet.bottleneck.0.groupnorm_merged.weight'],
        state_dict[model_key]['unet.bottleneck.0.groupnorm_merged.bias']
    ])
    res_block_0.conv_merged.set_weights([
        state_dict[model_key]['unet.bottleneck.0.conv_merged.weight'],
        state_dict[model_key]['unet.bottleneck.0.conv_merged.bias']
    ])

    # Attention block 1
    attn_block_1 = bottleneck_layers[1]
    attn_block_1.groupnorm.set_weights([
        state_dict[model_key]['unet.bottleneck.1.groupnorm.weight'],
        state_dict[model_key]['unet.bottleneck.1.groupnorm.bias']
    ])
    attn_block_1.conv_input.set_weights([
        state_dict[model_key]['unet.bottleneck.1.conv_input.weight'],
        state_dict[model_key]['unet.bottleneck.1.conv_input.bias']
    ])
    attn_block_1.layernorm_1.set_weights([
        state_dict[model_key]['unet.bottleneck.1.layernorm_1.weight'],
        state_dict[model_key]['unet.bottleneck.1.layernorm_1.bias']
    ])
    attn_block_1.attention1.in_proj_bias.set_weights([
        state_dict[model_key]['unet.bottleneck.1.attention_1.in_proj.weight']
    ])
    attn_block_1.attention1.out_proj.set_weights([
        state_dict[model_key]['unet.bottleneck.1.attention_1.out_proj.weight'],
        state_dict[model_key]['unet.bottleneck.1.attention_1.out_proj.bias']
    ])
    attn_block_1.layernorm_2.set_weights([
        state_dict[model_key]['unet.bottleneck.1.layernorm_2.weight'],
        state_dict[model_key]['unet.bottleneck.1.layernorm_2.bias']
    ])
    attn_block_1.attention2.q_proj.set_weights([
        state_dict[model_key]['unet.bottleneck.1.attention_2.q_proj.weight']
    ])
    attn_block_1.attention2.k_proj.set_weights([
        state_dict[model_key]['unet.bottleneck.1.attention_2.k_proj.weight']
    ])
    attn_block_1.attention2.v_proj.set_weights([
        state_dict[model_key]['unet.bottleneck.1.attention_2.v_proj.weight']
    ])
    attn_block_1.attention2.out_proj.set_weights([
        state_dict[model_key]['unet.bottleneck.1.attention_2.out_proj.weight'],
        state_dict[model_key]['unet.bottleneck.1.attention_2.out_proj.bias']
    ])
    attn_block_1.layernorm_3.set_weights([
        state_dict[model_key]['unet.bottleneck.1.layernorm_3.weight'],
        state_dict[model_key]['unet.bottleneck.1.layernorm_3.bias']
    ])
    attn_block_1.linear_geglu_1.set_weights([
        state_dict[model_key]['unet.bottleneck.1.linear_geglu_1.weight'],
        state_dict[model_key]['unet.bottleneck.1.linear_geglu_1.bias']
    ])
    attn_block_1.linear_geglu_2.set_weights([
        state_dict[model_key]['unet.bottleneck.1.linear_geglu_2.weight'],
        state_dict[model_key]['unet.bottleneck.1.linear_geglu_2.bias']
    ])
    attn_block_1.conv_output.set_weights([
        state_dict[model_key]['unet.bottleneck.1.conv_output.weight'],
        state_dict[model_key]['unet.bottleneck.1.conv_output.bias']
    ])

    # Residual block 2
    res_block_2 = bottleneck_layers[2]
    res_block_2.groupnorm.set_weights([
        state_dict[model_key]['unet.bottleneck.2.groupnorm_feature.weight'],
        state_dict[model_key]['unet.bottleneck.2.groupnorm_feature.bias']
    ])
    res_block_2.conv_feature.set_weights([
        state_dict[model_key]['unet.bottleneck.2.conv_feature.weight'],
        state_dict[model_key]['unet.bottleneck.2.conv_feature.bias']
    ])
    res_block_2.linear_time.set_weights([
        state_dict[model_key]['unet.bottleneck.2.linear_time.weight'],
        state_dict[model_key]['unet.bottleneck.2.linear_time.bias']
    ])
    res_block_2.groupnorm_merged.set_weights([
        state_dict[model_key]['unet.bottleneck.2.groupnorm_merged.weight'],
        state_dict[model_key]['unet.bottleneck.2.groupnorm_merged.bias']
    ])
    res_block_2.conv_merged.set_weights([
        state_dict[model_key]['unet.bottleneck.2.conv_merged.weight'],
        state_dict[model_key]['unet.bottleneck.2.conv_merged.bias']
    ])

    # UNET Decoders
    unet_decoders_tf_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11] # Indices in diffusion.unet.decoders
    for dec_idx in unet_decoders_tf_indices:
        decoder_layers = diffusion.unet.decoders[dec_idx].layers_list
        prefix = 'unet.decoders.' + str(dec_idx)

        # First layer in decoder block is always UNET_ResidualBlock
        res_block = decoder_layers[0]
        res_block.groupnorm.set_weights([
            state_dict[model_key][prefix + '.0.groupnorm_feature.weight'],
            state_dict[model_key][prefix + '.0.groupnorm_feature.bias']
        ])
        res_block.conv_feature.set_weights([
            state_dict[model_key][prefix + '.0.conv_feature.weight'],
            state_dict[model_key][prefix + '.0.conv_feature.bias']
        ])
        res_block.linear_time.set_weights([
            state_dict[model_key][prefix + '.0.linear_time.weight'],
            state_dict[model_key][prefix + '.0.linear_time.bias']
        ])
        res_block.groupnorm_merged.set_weights([
            state_dict[model_key][prefix + '.0.groupnorm_merged.weight'],
            state_dict[model_key][prefix + '.0.groupnorm_merged.bias']
        ])
        res_block.conv_merged.set_weights([
            state_dict[model_key][prefix + '.0.conv_merged.weight'],
            state_dict[model_key][prefix + '.0.conv_merged.bias']
        ])
        if isinstance(res_block.residual_layer, tf.keras.layers.Conv2D):
                res_block.residual_layer.set_weights([
                    state_dict[model_key][prefix + '.0.residual_layer.weight'],
                    state_dict[model_key][prefix + '.0.residual_layer.bias']
                ])

        # Check for UNET_AttentionBlock (index 1 in decoder_layers if present)
        if len(decoder_layers) > 1 and isinstance(decoder_layers[1], UNET_AttentionBlock):
            attn_block = decoder_layers[1]
            attn_prefix = prefix + '.1'
            attn_block.groupnorm.set_weights([
                state_dict[model_key][attn_prefix + '.groupnorm.weight'],
                state_dict[model_key][attn_prefix + '.groupnorm.bias']
            ])
            attn_block.conv_input.set_weights([
                state_dict[model_key][attn_prefix + '.conv_input.weight'],
                state_dict[model_key][attn_prefix + '.conv_input.bias']
            ])
            attn_block.layernorm_1.set_weights([
                state_dict[model_key][attn_prefix + '.layernorm_1.weight'],
                state_dict[model_key][attn_prefix + '.layernorm_1.bias']
            ])
            attn_block.attention1.in_proj_bias.set_weights([
                state_dict[model_key][attn_prefix + '.attention_1.in_proj.weight']
            ])
            attn_block.attention1.out_proj.set_weights([
                state_dict[model_key][attn_prefix + '.attention_1.out_proj.weight'],
                state_dict[model_key][attn_prefix + '.attention_1.out_proj.bias']
            ])
            attn_block.layernorm_2.set_weights([
                state_dict[model_key][attn_prefix + '.layernorm_2.weight'],
                state_dict[model_key][attn_prefix + '.layernorm_2.bias']
            ])
            attn_block.attention2.q_proj.set_weights([
                state_dict[model_key][attn_prefix + '.attention_2.q_proj.weight']
            ])
            attn_block.attention2.k_proj.set_weights([
                state_dict[model_key][attn_prefix + '.attention_2.k_proj.weight']
            ])
            attn_block.attention2.v_proj.set_weights([
                state_dict[model_key][attn_prefix + '.attention_2.v_proj.weight']
            ])
            attn_block.attention2.out_proj.set_weights([
                state_dict[model_key][attn_prefix + '.attention_2.out_proj.weight'],
                state_dict[model_key][attn_prefix + '.attention_2.out_proj.bias']
            ])
            attn_block.layernorm_3.set_weights([
                state_dict[model_key][attn_prefix + '.layernorm_3.weight'],
                state_dict[model_key][attn_prefix + '.layernorm_3.bias']
            ])
            attn_block.linear_geglu_1.set_weights([
                state_dict[model_key][attn_prefix + '.linear_geglu_1.weight'],
                state_dict[model_key][attn_prefix + '.linear_geglu_1.bias']
            ])
            attn_block.linear_geglu_2.set_weights([
                state_dict[model_key][attn_prefix + '.linear_geglu_2.weight'],
                state_dict[model_key][attn_prefix + '.linear_geglu_2.bias']
            ])
            attn_block.conv_output.set_weights([
                state_dict[model_key][attn_prefix + '.conv_output.weight'],
                state_dict[model_key][attn_prefix + '.conv_output.bias']
            ])

        # Check for Upsample (index 2 in decoder_layers if present)
        if len(decoder_layers) > 2 and isinstance(decoder_layers[2], Upsample):
            upsample_layer = decoder_layers[2]
            upsample_prefix = prefix + '.2'
            upsample_layer.conv.set_weights([
                state_dict[model_key][upsample_prefix + '.conv.weight'],
                state_dict[model_key][upsample_prefix + '.conv.bias']
            ])

    # Diffusion final layer
    diffusion.final.groupnorm.set_weights([
        state_dict[model_key]['final.groupnorm.weight'],
        state_dict[model_key]['final.groupnorm.bias']
    ])
    diffusion.final.conv.set_weights([
        state_dict[model_key]['final.conv.weight'],
        state_dict[model_key]['final.conv.bias']
    ])

    # Assign weights to CLIP
    model_key = 'clip'
    clip.embedding.token_embedding.set_weights([
        state_dict[model_key]['embedding.token_embedding.weight']
    ])
    # Position embedding is a tf.Variable, can be assigned directly
    clip.embedding.position_embedding.assign(state_dict[model_key]['embedding.position_embedding'])

    for i in range(12):
        clip_layer = clip.layers[i]
        prefix = 'layers.' + str(i)

        clip_layer.layer_norm1.set_weights([
            state_dict[model_key][prefix + '.layernorm_1.weight'],
            state_dict[model_key][prefix + '.layernorm_1.bias']
        ])
        clip_layer.attn.in_proj_bias.set_weights([
            state_dict[model_key][prefix + '.attention.in_proj.weight'],
            state_dict[model_key][prefix + '.attention.in_proj.bias']
        ])
        clip_layer.attn.out_proj.set_weights([
            state_dict[model_key][prefix + '.attention.out_proj.weight'],
            state_dict[model_key][prefix + '.attention.out_proj.bias']
        ])
        clip_layer.layer_norm2.set_weights([
            state_dict[model_key][prefix + '.layernorm_2.weight'],
            state_dict[model_key][prefix + '.layernorm_2.bias']
        ])
        clip_layer.linear1.set_weights([
            state_dict[model_key][prefix + '.linear_1.weight'],
            state_dict[model_key][prefix + '.linear_1.bias']
        ])
        clip_layer.linear2.set_weights([
            state_dict[model_key][prefix + '.linear_2.weight'],
            state_dict[model_key][prefix + '.linear_2.bias']
        ])

    clip.layer_norm.set_weights([
        state_dict[model_key]['layernorm.weight'],
        state_dict[model_key]['layernorm.bias']
    ])

    return {
        'clip': clip,
        'encoder': encoder,
        'decoder': decoder,
        'diffusion': diffusion,
    }

import tensorflow as tf
import numpy as np
from tqdm import tqdm

WIDTH = 512
HEIGHT = 512
LATENTS_WIDTH = WIDTH // 8
LATENTS_HEIGHT = HEIGHT // 8

def generate_tf(
    prompt,
    models,
    tokenizer,
    sampler_name="ddpm",
    n_inference_steps=50,
    strength=0.8,
    cfg_scale=7.5,
    do_cfg=True,
    uncond_prompt="",
    seed=None,
    input_image=None,
    device="/GPU:0"
):
    if not 0 < strength <= 1:
        raise ValueError("strength must be between 0 and 1")

    # ----------------------------
    # Random generator
    # ----------------------------
    if seed is None:
        generator = tf.random.Generator.from_non_deterministic_state()
    else:
        generator = tf.random.Generator.from_seed(seed)

    # ----------------------------
    # TEXT ENCODING (CLIP)
    # ----------------------------
    with tf.device(device):
        clip = models["clip"]

        if do_cfg:
            cond_tokens = tokenizer.batch_encode_plus(
                [prompt],
                padding="max_length",
                max_length=77,
                return_tensors=None
            )["input_ids"]

            uncond_tokens = tokenizer.batch_encode_plus(
                [uncond_prompt or ""],
                padding="max_length",
                max_length=77,
                return_tensors=None
            )["input_ids"]

            cond_tokens = tf.convert_to_tensor(cond_tokens, dtype=tf.int32)
            uncond_tokens = tf.convert_to_tensor(uncond_tokens, dtype=tf.int32)

            cond_context = clip(cond_tokens, training=False)
            uncond_context = clip(uncond_tokens, training=False)

            context = tf.concat([cond_context, uncond_context], axis=0)

        else:
            tokens = tokenizer.batch_encode_plus(
                [prompt],
                padding="max_length",
                max_length=77,
                return_tensors=None
            )["input_ids"]

            tokens = tf.convert_to_tensor(tokens, dtype=tf.int32)
            context = clip(tokens, training=False)

    # ----------------------------
    # SAMPLER
    # ----------------------------
    if sampler_name == "ddpm":
        sampler = DDPMSampler(generator)
        sampler.set_inference_timesteps(n_inference_steps)
    else:
        raise ValueError(f"Unknown sampler {sampler_name}")

    latents_shape = (1, 4, LATENTS_HEIGHT, LATENTS_WIDTH)

    # ----------------------------
    # IMAGE → LATENTS (IMG2IMG)
    # ----------------------------
    with tf.device(device):
        if input_image is not None:
            encoder = models["encoder"]

            img = input_image.resize((WIDTH, HEIGHT))
            img = np.array(img).astype(np.float32)
            img = rescale(img, (0, 255), (-1, 1))

            img = tf.convert_to_tensor(img)
            img = tf.expand_dims(img, 0)              # (1, H, W, C)
            img = tf.transpose(img, (0, 3, 1, 2))     # (1, C, H, W)

            noise = generator.normal(latents_shape)
            latents = encoder([img, noise], training=False)

            sampler.set_strength(strength)
            latents = sampler.add_noise(latents, sampler.timesteps[0])

        else:
            latents = generator.normal(latents_shape)

    # ----------------------------
    # DIFFUSION LOOP
    # ----------------------------
    diffusion = models["diffusion"]

    for timestep in tqdm(sampler.timesteps):
        with tf.device(device):
            time_embedding = get_time_embedding(timestep)

            model_input = latents

            if do_cfg:
                model_input = tf.repeat(model_input, repeats=2, axis=0)

            model_output = diffusion(
                model_input, context, time_embedding, # Passing arguments explicitly
                training=False
            )

            if do_cfg:
                output_cond, output_uncond = tf.split(model_output, 2, axis=0)
                model_output = (
                    cfg_scale * (output_cond - output_uncond) + output_uncond
                )

            latents = sampler.step(timestep, latents, model_output)

    # ----------------------------
    # DECODE
    # ----------------------------
    decoder = models["decoder"]
    with tf.device(device):
        images = decoder(latents, training=False)

        images = rescale(images, (-1, 1), (0, 255), clamp=True)
        images = tf.transpose(images, (0, 2, 3, 1))
        images = tf.cast(images, tf.uint8)

    return images[0].numpy()

def rescale(x, old_range, new_range, clamp=False):
    old_min, old_max = old_range
    new_min, new_max = new_range
    x -= old_min
    x *= (new_max - new_min) / (old_max - old_min)
    x += new_min
    if clamp:
        x = tf.clip_by_value(x, new_min, new_max)
    return x

def get_time_embedding(timestep):
    # freqs: (160,)
  freqs = tf.pow(
    10000.0,
    -tf.range(start=0, limit=160, dtype=tf.float32) / 160.0
      )

      # x: (1, 160)
  timestep = tf.cast(timestep, tf.float32)
  x = tf.reshape(timestep, (1, 1)) * tf.reshape(freqs, (1, 160))

      # (1, 320)
  return tf.concat([tf.cos(x), tf.sin(x)], axis=-1)

pip install pytorch_lightning

import torch
from PIL import Image
from transformers import CLIPTokenizer

device = "cuda" if torch.cuda.is_available() else "cpu"

# Load the tokenizer
tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-large-patch14")

# Preload models

models = preload_models_from_standard_weights(cache_path, device)

image_generated =  generate_tf(
    prompt  = "A beautiful car",
    uncond_prompt=None, # Negative prompt, Example the cat is not in the sofa
    input_image=None,
    strength=0.8, # If we are doing Image to Image, how much attention to be given to the first image
    do_cfg=True,
    cfg_scale=7.5, # Classifier free Generation weight
    sampler_name="ddpm",
    n_inference_steps=50,
    models=models,
    seed=None,
    device=device,
    tokenizer=tokenizer,
)

img = Image.fromarray(image_generated)
img.show()