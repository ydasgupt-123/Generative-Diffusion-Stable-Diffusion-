# -*- coding: utf-8 -*-
"""Diffusion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J8qv6VdEQoHLd-ekpFcqSnE54oshUplu
"""

pip install datasets sentence-transformers

pip install pillow

pip install tensorflow keras tensorflow-keras

pip install pandas

pip install matplotlib

import tensorflow as tf
from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input
from tensorflow.keras.preprocessing import image
import numpy as np
import os
from PIL import Image

# Load a pre-trained model for feature extraction
feature_model = MobileNetV2(weights='imagenet', include_top=False, pooling='avg')

def extract_features(img_input):
    """Loads and preprocesses an image (from path, PIL Image, or dictionary), then extracts 384-D features."""
    if isinstance(img_input, str):
        img = Image.open(img_input).convert('RGB')
    elif isinstance(img_input, Image.Image):
        img = img_input.convert('RGB')
    elif isinstance(img_input, dict) and 'bytes' in img_input:
        img = Image.open(io.BytesIO(img_input['bytes'])).convert('RGB')
    else:
        raise TypeError("Input must be a file path, PIL Image, or dict with 'bytes' key")

    # Resize to model input size
    img = img.resize((224, 224))
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array = preprocess_input(img_array)

    # Extract features
    features = feature_model.predict(img_array)

    # Flatten to 1D NumPy array
    return features.flatten()

import tensorflow as tf
import keras
from keras import layers, Sequential
import numpy as np
import os
from PIL import Image

decoder_model = tf.keras.Sequential([
    layers.Input(shape=(1280,)),

    # Project latent vector
    layers.Dense(7 * 7 * 512),
    layers.Reshape((7, 7, 512)),

    # 16 → 32
    layers.Conv2DTranspose(512, 3, strides=2, padding='same', activation='relu'),

    # 32 → 64
    layers.Conv2DTranspose(256, 3, strides=2, padding='same', activation='relu'),

    # 64 → 128
    layers.Conv2DTranspose(128, 3, strides=2, padding='same', activation='relu'),

    # 128 → 256
    layers.Conv2DTranspose(64, 3, strides=2, padding='same', activation='relu'),

    # 256 → 512
    layers.Conv2DTranspose(32, 3, strides=2, padding='same', activation='relu'),

    # Final RGB output
    layers.Conv2DTranspose(3, 3, strides=1, padding='same', activation='relu')
])


decoder_model.compile(optimizer='adam', loss='mse')

from datasets import load_dataset
import pandas as pd
from io import BytesIO
from PIL import Image

training_data = load_dataset("microsoft/cats_vs_dogs")
training_data = training_data['train'].to_pandas()

input_sequence =[]
output_sequence = []

for i in range(2000):
  img_dict = training_data['image'].iloc[i]
  img = Image.open(BytesIO(img_dict["bytes"])).convert("RGB")
  input_sequence.append(extract_features(img))
  img=img.resize((224,224))
  img=np.array(img)
  output_sequence.append(img)

input_sequence = np.array(input_sequence)
output_sequence = np.array(output_sequence)

decoder_model.fit(input_sequence, output_sequence, epochs=10, batch_size=32)

import matplotlib.pyplot as plt
k = (extract_features('/content/Rancho Test.jpg'))
print(k)

p= decoder_model.predict(np.array([k]))
plt.imshow(p[0].astype('uint8'))